{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy matplotlib pandas pillow scikit-learn gempy gempy-viewer autograd pyvista vtk tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter import ttk,font\n",
    "from PIL import Image, ImageTk\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import gempy as gp\n",
    "import gempy_viewer as gpv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from autograd import numpy as np\n",
    "import numpy as npy\n",
    "import pyvista as pv\n",
    "from autograd import elementwise_grad as egrad\n",
    "import vtk\n",
    "import warnings\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from scipy.constants import G\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import Rbf\n",
    "from sympy import symbols, diff\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import cmcrameri as cmc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "class GeologicalModelApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.geometry(\"1920x1080\")\n",
    "        self.menu_font = font.Font(family='Arial', size=16)\n",
    "        self.root.option_add('*Menu.font', self.menu_font)\n",
    "        self.menubar = tk.Menu(root,)\n",
    "        root.config(menu=self.menubar)\n",
    "        self.function_menu = tk.Menu(self.menubar, tearoff=0)\n",
    "        self.menubar.add_cascade(label=\"Model generator\", menu=self.function_menu)\n",
    "        self.process_menu = tk.Menu(self.menubar, tearoff=0)\n",
    "        self.menubar.add_cascade(label=\"Model Process\", menu=self.process_menu)\n",
    "        self.interpolation_menu = tk.Menu(self.menubar, tearoff=0)\n",
    "        self.menubar.add_cascade(label=\"Interpolation and Comparison\", menu=self.interpolation_menu)\n",
    "        self.menubar.entryconfig(\"Model generator\", font=self.menu_font)\n",
    "        self.function_menu.add_command(label=\"save/load model\", command=lambda: self.show_tab(self.tab1, 'Save/Load Data'))\n",
    "        self.function_menu.add_command(label=\"Create GemPy Model\", command=lambda: self.show_tab(self.tab2, 'Create GemPy Model'))\n",
    "        self.function_menu.add_command(label=\"Drawing\", command=lambda: self.show_tab(self.tab3, 'Drawing'))\n",
    "        self.function_menu.add_command(label=\"2D and 3D Plot\", command=lambda: self.show_tab(self.tab4, '2D and 3D Plot'))\n",
    "        self.process_menu.add_command(label=\"VTK to Scalar Field\", command=lambda: self.show_tab(self.tab5, 'VTK to Scalar Field'))\n",
    "        self.interpolation_menu.add_command(label=\"Implicit Interpolation\", command=lambda: self.show_tab(self.tab6, 'Implicit Interpolation'))\n",
    "        self.process_menu.add_command(label=\"Cross-Validation\", command=lambda: self.show_tab(self.tab7, 'Cross-Validation'))\n",
    "        self.interpolation_menu.add_command(label=\"Scalar Field Comparison\", command=lambda: self.show_tab(self.tab8, 'Scalar Field Comparison'))\n",
    "        self.process_menu.add_command(label=\"Gravity Simulation\", command=lambda: self.show_tab(self.tab11, 'Gravity Simulation'))        \n",
    "        self.notebook = ttk.Notebook(root)\n",
    "        self.notebook.pack(fill='both', expand=True)\n",
    "\n",
    "        self.style = ttk.Style()\n",
    "        self.style.configure('TNotebook.Tab', font=('Arial', '16'))\n",
    "        self.style.configure('TButton', font=('Arial', 14), padding=10, borderwidth=10, relief=\"flat\", background=\"#C0E6A7\")\n",
    " \n",
    "        \n",
    "        self.tab1 = ttk.Frame(self.notebook)\n",
    "        self.tab2 = ttk.Frame(self.notebook)\n",
    "        self.tab3 = ttk.Frame(self.notebook)\n",
    "        self.tab4 = ttk.Frame(self.notebook)\n",
    "        self.tab5 = ttk.Frame(self.notebook)\n",
    "        self.tab6 = ttk.Frame(self.notebook)\n",
    "       \n",
    "        self.auto_corr = tk.IntVar()\n",
    "        self.auto_corr.set(0)\n",
    "        self.show_drift_var = tk.IntVar()\n",
    "        self.show_drift_var.set(0)\n",
    "        self.only_show_data = tk.IntVar()\n",
    "        self.only_show_data.set(0)\n",
    "        self.name = {}\n",
    "        self.name_saved = []\n",
    "        self.x_pos = {}\n",
    "        self.x_pos_saved = []\n",
    "        self.y_pos = {}\n",
    "        self.y_pos_saved = []\n",
    "        self.x_points = []\n",
    "        self.y_points = []\n",
    "        self.vertices_df = None\n",
    "        self.normals_df = None\n",
    "\n",
    "        # Define font styles\n",
    "        self.title_font = (\"Helvetica\", 16, \"bold\")\n",
    "        self.label_font = (\"Helvetica\", 14)\n",
    "        self.label_font_s = (\"Helvetica\", 10)\n",
    "        self.entry_font = (\"Helvetica\", 14)\n",
    "        self.button_font = (\"Helvetica\", 14, \"bold\")\n",
    "\n",
    "        # Tab 1: Save/Load Data\n",
    "        self.save_button = ttk.Button(self.tab1, text=\"Save Data\",  command=self.save_data, style='TButton')\n",
    "        self.save_button.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_button = ttk.Button(self.tab1, text=\"Load Data\",  command=self.load_data, style='TButton')\n",
    "        self.load_button.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "        \n",
    "        self.save_model_button = ttk.Button(self.tab1, text=\"Save Gempy Model\",  command=self.save_gempy_model, style='TButton')\n",
    "        self.save_model_button.grid(row=0, column=2, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        # Tab 2: Model Parameters\n",
    "        self.Layer_number_label = tk.Label(self.tab2, text=\"Please enter the layer number:\", font=self.label_font)\n",
    "        self.layer_number_entry = tk.Entry(self.tab2, font=self.entry_font)\n",
    "        self.layer_number_confirm = ttk.Button(self.tab2, text=\"Confirm\", style='TButton', command=self.set_layer_name)\n",
    "\n",
    "        self.Layer_number_label.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "        self.layer_number_entry.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "        self.layer_number_confirm.grid(row=0, column=2, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.en = tk.Checkbutton(self.tab2, text=\"Enable auto correction\", variable=self.auto_corr, onvalue=1, offvalue=0, font=self.label_font)\n",
    "        self.en.grid(row=1, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        # Tab 3: Drawing\n",
    "        self.draw_frame = tk.Frame(self.tab3)\n",
    "        self.draw_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        # Tab 4: 2D Plot\n",
    "        self.plot_2d_frame = tk.Frame(self.tab4)\n",
    "        self.plot_2d_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.plot_3d_button = ttk.Button(self.plot_2d_frame, text=\"Show 3D Plot\", style='TButton', command=self.plot_3d)\n",
    "        self.plot_3d_button.pack(pady=10)\n",
    "\n",
    "        # Tab 5: VTK Operations\n",
    "        self.vtk_operations_frame = tk.Frame(self.tab5)\n",
    "        self.vtk_operations_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_vtk_button = ttk.Button(self.vtk_operations_frame, text=\"Load VTK File\", style='TButton', command=self.load_vtk_file)\n",
    "        self.load_vtk_button.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "        \n",
    "        self.compute_scalar_button = ttk.Button(self.vtk_operations_frame, text=\"Compute Scalar Field\", style='TButton', command=self.compute_scalar_field)\n",
    "        self.compute_scalar_button.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "        \n",
    "        self.save_scalar_button = ttk.Button(self.vtk_operations_frame, text=\"Save Scalar Field\", style='TButton', command=self.save_scalar_field)\n",
    "        self.save_scalar_button.place(relx=0.68, rely=0.19, anchor='center')\n",
    "\n",
    "        self.vertices_file_path = tk.StringVar()\n",
    "        self.normals_file_path = tk.StringVar()\n",
    "        self.scalar_file_path = tk.StringVar()\n",
    "\n",
    "        tk.Label(self.vtk_operations_frame, text=\"Vertices File Path:\", font=self.label_font).grid(row=1, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.vertices_entry = tk.Entry(self.vtk_operations_frame, textvariable=self.vertices_file_path, font=self.entry_font, width=50)\n",
    "        self.vertices_entry.grid(row=1, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.vtk_operations_frame, text=\"Normals File Path:\", font=self.label_font).grid(row=2, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.normals_entry = tk.Entry(self.vtk_operations_frame, textvariable=self.normals_file_path, font=self.entry_font, width=50)\n",
    "        self.normals_entry.grid(row=2, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.vtk_operations_frame, text=\"Scalar Field File Path:\", font=self.label_font).grid(row=3, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.scalar_entry = tk.Entry(self.vtk_operations_frame, textvariable=self.scalar_file_path, font=self.entry_font, width=50)\n",
    "        self.scalar_entry.grid(row=3, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        self.save_excel_button = ttk.Button(self.vtk_operations_frame, text=\"Save to Excel\", style='TButton', command=self.save_vtk_to_excel)\n",
    "        self.save_excel_button.grid(row=4, column=0, padx=10, pady=10, sticky='w')\n",
    "        self.save_excel_button.place(relx=0.90, rely=0.19, anchor='center')\n",
    "\n",
    "        self.pv_canvas_frame = tk.Frame(self.tab5, width=800, height=600)\n",
    "        self.pv_canvas_frame.grid(row=5, column=0, columnspan=3, padx=10, pady=10)\n",
    "\n",
    "        self.plotter = None\n",
    "        self.init_pyvista_plotter()\n",
    "\n",
    "        # Tab 6: RBF Interpolation\n",
    "        self.rbf_frame = tk.Frame(self.tab6)\n",
    "        self.rbf_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_surface_button = ttk.Button(self.rbf_frame, text=\"Load Surface Points\", style='TButton', command=self.load_surface_points)\n",
    "        self.load_surface_button.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "        \n",
    "        self.load_orientation_button = ttk.Button(self.rbf_frame, text=\"Load Orientation Points\", style='TButton', command=self.load_orientation_points)\n",
    "        self.load_orientation_button.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_orientations_button = ttk.Button(self.rbf_frame, text=\"Load Orientations\", style='TButton', command=self.load_orientations)\n",
    "        self.load_orientations_button.grid(row=0, column=2, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Sampling method:\", font=self.label_font).grid(row=1, column=2, padx=10, pady=5, sticky='w')\n",
    "        self.Sampling_method = ttk.Combobox(self.rbf_frame, font=self.entry_font, values=[\n",
    "            'Random','Systematic' ,'Statistic(K-means)',])\n",
    "        self.Sampling_method.grid(row=1, column=3, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Number of Sample Surface Points:\", font=self.label_font).grid(row=1, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.num_surface_points = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.num_surface_points.grid(row=1, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Number of Sample Orientation Points:\", font=self.label_font).grid(row=2, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.num_orientation_points = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.num_orientation_points.grid(row=2, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Kernel Function:\", font=self.label_font).grid(row=3, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.kernel_function = ttk.Combobox(self.rbf_frame, font=self.entry_font, values=[\n",
    "            'cubic', 'gaussian', 'quintic', 'cubic_covariance', 'multiquadric', 'linear', \n",
    "            'thin_plate', 'inverse_quadratic', 'inverse_multiquadric', 'gaussian_covariance', \n",
    "            'ga_cub_cov', 'ga_cub'])\n",
    "        self.kernel_function.grid(row=3, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Drift Type:\", font=self.label_font).grid(row=4, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.drift_type = ttk.Combobox(self.rbf_frame, font=self.entry_font, values=[\n",
    "            'None', 'First Order Polynomial','Second Order Polynomial', 'Ellipsoid', 'Dome Shaped','2D Custom','3D Custom'])\n",
    "        self.drift_type.grid(row=4, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        self.load_drift_scalar_button = ttk.Button(self.rbf_frame, text=\"Load Drift Scalar Field\", style='TButton', command=self.load_drift_scalar)\n",
    "        self.load_drift_scalar_button.grid(row=4, column=2, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Drift scaling factor:\", font=self.label_font).grid(row=4, column=3, padx=10, pady=5, sticky='w')\n",
    "        self.scaling_factor = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.scaling_factor.grid(row=4, column=4, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Epsilon for gaussian kernel (default is calculated):\", font=self.label_font).grid(row=5, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.epsilon_entry = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.epsilon_entry.grid(row=5, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Range for covariance kernel (default is calculated):\", font=self.label_font).grid(row=6, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.range_entry = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.range_entry.grid(row=6, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Parameter a:the width of the salt dome or x coordinate of sphere center  (default:1)\", font=self.label_font).grid(row=7, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.param_a_entry = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.param_a_entry.grid(row=7, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Parameter b:flatness of the top of the salt dome or y coordinate of sphere center (default:1):\", font=self.label_font).grid(row=8, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.param_b_entry = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.param_b_entry.grid(row=8, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Parameter c:height of the salt dome or z coordinate of sphere center (default:1):\", font=self.label_font).grid(row=9, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.param_c_entry = tk.Entry(self.rbf_frame, font=self.entry_font)\n",
    "        self.param_c_entry.grid(row=9, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        self.compute_rbf_button = ttk.Button(self.rbf_frame, text=\"Compute RBF Interpolation\", style='TButton', command=self.compute_rbf_interpolation)\n",
    "        self.compute_rbf_button.grid(row=10, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.show_drift = tk.Checkbutton(self.rbf_frame, text=\"show drift contribution\", variable=self.show_drift_var, onvalue=1, offvalue=0, font=self.label_font)\n",
    "        self.show_drift.grid(row=11, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.show_data = tk.Checkbutton(self.rbf_frame, text=\"only show data\", variable=self.only_show_data, onvalue=1, offvalue=0, font=self.label_font)\n",
    "        self.show_data.grid(row=12, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        tk.Label(self.rbf_frame, text=\"Model color:\", font=self.label_font).grid(row=13, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.model_color = ttk.Combobox(self.rbf_frame, font=self.entry_font, values=[\n",
    "            'red', 'green', 'blue', 'orange', 'purple', 'black', 'darkgrey'])\n",
    "        self.model_color.grid(row=13, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "\n",
    "        # Tab 7: Cross-Validation\n",
    "        self.tab7 = ttk.Frame(self.notebook)\n",
    "        self.create_cross_validation_tab()\n",
    "\n",
    "        # Tab 8：Scalar Field Comparison\n",
    "        self.scalar1 = None\n",
    "        self.scalar2 = None\n",
    "        self.use_flip = tk.BooleanVar()\n",
    "        self.use_reverse = tk.BooleanVar()\n",
    "        self.tab8 = ttk.Frame(self.notebook)\n",
    "        self.create_scalar_comparison_tab()\n",
    "\n",
    "        # Tab 9 : About\n",
    "        self.tab9 = ttk.Frame(self.notebook)\n",
    "\n",
    "        # Tab 11: Gravity simulation\n",
    "        self.tab11 = ttk.Frame(self.notebook)\n",
    "        self.gravity_simulation_frame = tk.Frame(self.tab11)\n",
    "        self.gravity_simulation_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "        self.load_vtk_button = ttk.Button(self.gravity_simulation_frame, text=\"Load VTK File\", style='TButton', command=self.load_vtk_to_gravity)\n",
    "        self.load_vtk_button.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.gravity_fwd_button = ttk.Button(self.gravity_simulation_frame, text=\"Compute Gravity Forward Scalar Field\", style='TButton', command=self.gravity_fwd_2d)\n",
    "        self.gravity_fwd_button.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.gravity_canvas_frame = tk.Frame(self.tab11)\n",
    "        self.gravity_canvas_frame.grid(row=4, column=0, padx=10, pady=10, columnspan=4)\n",
    "\n",
    "        # Load the image\n",
    "        image_path = \"workflow.png\"  # Replace with the actual path to your image\n",
    "        image = Image.open(image_path)\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        text_label = tk.Label(self.tab9, text=\"This software is designed for kernel based implicit geological modeling.\\n\"\n",
    "                         \"The functions include:\\n\"\n",
    "                         \"1. Create Gempy model by drawing.\\n\"\n",
    "                         \"2. Save and load existing drawing model, or save GemPy model as .vtp format \"\n",
    "                         \"(only for single layer model).\\n\"\n",
    "                         \"3. Visualize created GemPy model in 2D and 3D.\\n\"\n",
    "                         \"4. Create scalar field for .vtp or .vtk model, and save the result as .npy format. \"\n",
    "                         \"The vertices and normals of model also can be saved to excel file.\\n\"\n",
    "                         \"5. Implicit modeling using surface points and orientations. The sampling density can be changed.\\n\"\n",
    "                         \"6. Cross-validation method for finding the best parameter of kernels and drift.\\n\"\n",
    "                         \"7. Scalar field comparison using cosine difference.\\n\"\n",
    "                         \"The meaning of parameter a, b, c in drift function:\\n\"\n",
    "                         \"For dome shaped drift:\\n\"\n",
    "                         \"Parameter 𝑎: This parameter controls the width of the salt dome. A larger value of 𝑎 results in a wider lateral extension of the salt dome, while a smaller value of 𝑎 leads to a narrower width.\\n\"\n",
    "                         \"Parameter 𝑏: This parameter governs the flatness of the salt dome’s top. A larger value of 𝑏 makes the top more flat and extended, whereas a smaller value of 𝑏 leads to a steeper top.\\n\"\n",
    "                         \"Parameter 𝑐: This parameter determines the height of the salt dome. A larger value of 𝑐 results in a taller salt dome, while a smaller value of 𝑐 produces a lower structure.\\n\"\n",
    "                         \"For ellipsoid drift:\\n\"\n",
    "                         \"Parameter 𝑎: Semi axis along x-direction.\\n\"\n",
    "                         \"Parameter 𝑏: Semi axis along y-direction.\\n\"\n",
    "                         \"Parameter 𝑐: Semi axis along z-direction.\\n\",\n",
    "         font=self.label_font,anchor='w', justify='left',wraplength=1000)\n",
    "        # Create the image label\n",
    "        image_label = tk.Label(self.tab9, image=photo)\n",
    "\n",
    "        # Place the labels in the grid\n",
    "        text_label.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "        image_label.grid(row=0, column=1, padx=10, pady=10, sticky='e')\n",
    "\n",
    "        # Keep a reference to the image to prevent it from being garbage collected\n",
    "        image_label.image = photo\n",
    "\n",
    "        # Tab 10 : workflow\n",
    "        self.tab10 = ttk.Frame(self.notebook)\n",
    "        self.notebook.add(self.tab10, text='Workflow')\n",
    "        self.notebook.add(self.tab9, text='Help')\n",
    "        # Load the image\n",
    "        image_path = \"workguide_2.png\"  # Replace with the actual path to your image\n",
    "        image = Image.open(image_path)\n",
    "        original_width, original_height = image.size\n",
    "        scale_factor = min(1850 / original_width, 1080 / original_height)\n",
    "        new_width = int(original_width * scale_factor)\n",
    "        new_height = int(original_height * scale_factor)\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        \n",
    "        # Create the image label\n",
    "        image_label = tk.Label(self.tab10, image=photo)\n",
    "\n",
    "        # Place the labels in the grid\n",
    "        image_label.place(x=0, y=0, relwidth=1, relheight=1) \n",
    "\n",
    "        # Keep a reference to the image to prevent it from being garbage collected\n",
    "        image_label.image = photo\n",
    "        tab1_p = ImageTk.PhotoImage(Image.open(\"tab1.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab2_p = ImageTk.PhotoImage(Image.open(\"tab2.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab3_p = ImageTk.PhotoImage(Image.open(\"tab3.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab5_p = ImageTk.PhotoImage(Image.open(\"tab5.png\").resize((300, 77), Image.LANCZOS))\n",
    "        tab5_2_p = ImageTk.PhotoImage(Image.open(\"tab5_2.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab6_p = ImageTk.PhotoImage(Image.open(\"tab6.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab7_p = ImageTk.PhotoImage(Image.open(\"tab7.png\").resize((320, 55), Image.LANCZOS))\n",
    "        tab8_1_p = ImageTk.PhotoImage(Image.open(\"tab8_1.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab8_2_p = ImageTk.PhotoImage(Image.open(\"tab8_2.png\").resize((300, 55), Image.LANCZOS))\n",
    "        tab8_3_p = ImageTk.PhotoImage(Image.open(\"tab8_3.png\").resize((300, 55), Image.LANCZOS))\n",
    "\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.12, rely=0.47, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab2_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab2, 'Create GemPy Model'))\n",
    "        canvas.image = tab2_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.12, rely=0.55, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab3_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab3, 'Drawing'))\n",
    "        canvas.image = tab3_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.12, rely=0.63, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab1_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab1, 'Save/Load Data'))\n",
    "        canvas.image = tab1_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=70, highlightthickness=0)\n",
    "        canvas.place(relx=0.395, rely=0.31, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab5_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab5, 'VTK to Scalar Field'))\n",
    "        canvas.image = tab5_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.395, rely=0.39, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab5_2_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab5, 'VTK to Scalar Field'))\n",
    "        canvas.image = tab5_2_p\n",
    "        canvas = tk.Canvas(self.tab10, width=320, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.65, rely=0.195, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab7_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab7, 'Cross-Validation'))\n",
    "        canvas.image = tab7_p\n",
    "        canvas = tk.Canvas(self.tab10, width=320, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.89, rely=0.195, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab7_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab6, 'Implicit Interpolation'))\n",
    "        canvas.image = tab7_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.885, rely=0.86, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab5_2_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab5, 'VTK to Scalar Field'))\n",
    "        canvas.image = tab5_2_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.395, rely=0.67, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab8_1_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab8, 'Scalar Field Comparison'))\n",
    "        canvas.image = tab8_1_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.395, rely=0.75, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab8_2_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab8, 'Scalar Field Comparison'))\n",
    "        canvas.image = tab8_2_p\n",
    "        canvas = tk.Canvas(self.tab10, width=300, height=55, highlightthickness=0)\n",
    "        canvas.place(relx=0.395, rely=0.83, anchor='center') \n",
    "        canvas_image = canvas.create_image(0, 0, anchor='nw', image=tab8_3_p)\n",
    "        canvas.tag_bind(canvas_image, \"<Button-1>\", lambda event: self.show_tab(self.tab8, 'Scalar Field Comparison'))\n",
    "        canvas.image = tab8_3_p\n",
    "\n",
    "\n",
    "    def show_tab(self, tab, tab_text):\n",
    "        for t, text in [(self.tab1, 'Save/Load Data'), (self.tab2, 'Create GemPy Model'),\n",
    "                        (self.tab3, 'Drawing'), (self.tab4, '2D and 3D Plot'), (self.tab5, 'VTK to Scalar Field'),\n",
    "                        (self.tab6, 'Implicit Interpolation'), (self.tab7, 'Cross-Validation'), (self.tab8, 'Scalar Field Comparison'),\n",
    "                        (self.tab11, 'Gravity Simulation')]:\n",
    "            if text in [self.notebook.tab(nt, \"text\") for nt in self.notebook.tabs()]:\n",
    "                self.notebook.forget(t)\n",
    "        self.notebook.insert(0, tab, text=tab_text)\n",
    "        self.notebook.select(tab)\n",
    "\n",
    "\n",
    "\n",
    "    def init_pyvista_plotter(self):\n",
    "        pass  \n",
    "\n",
    "    def combine_funcs(self, *funcs):\n",
    "        def inner_combined_func(*args, **kwargs):\n",
    "            for f in funcs:\n",
    "                f(*args, **kwargs)\n",
    "        return inner_combined_func\n",
    "\n",
    "    def set_layer_name(self):\n",
    "        for i in range(int(self.layer_number_entry.get())):\n",
    "            tk.Label(self.tab2, text=\"Layer \" + str(i + 1) + \" name:\", font=self.label_font).grid(row=i + 2, column=0, padx=10, pady=5, sticky='w')\n",
    "            layer_name = f'Layer {str(i + 1)} name'\n",
    "            self.name[layer_name] = tk.Entry(self.tab2, font=self.entry_font)\n",
    "            self.name[layer_name].grid(row=i + 2, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "            tk.Label(self.tab2, text=\"Layer \" + str(i + 1) + \" X section Position:\", font=self.label_font).grid(row=i + 2, column=2, padx=10, pady=5, sticky='w')\n",
    "            self.x_pos[layer_name] = tk.Entry(self.tab2, font=self.entry_font)\n",
    "            self.x_pos[layer_name].grid(row=i + 2, column=3, padx=10, pady=5, sticky='w')\n",
    "\n",
    "            tk.Label(self.tab2, text=\"Layer \" + str(i + 1) + \" Y section Position:\", font=self.label_font).grid(row=i + 2, column=4, padx=10, pady=5, sticky='w')\n",
    "            self.y_pos[layer_name] = tk.Entry(self.tab2, font=self.entry_font)\n",
    "            self.y_pos[layer_name].grid(row=i + 2, column=5, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Button(self.tab2, text=\"Draw!\", font=self.button_font, command=self.combine_funcs(self.save_name, self.set_draw, lambda: self.show_tab(self.tab3, 'Drawing'))).grid(row=i + 3, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "    def save_name(self):\n",
    "        for i in range(int(self.layer_number_entry.get())):\n",
    "            self.name_saved.append(self.name[f'Layer {str(i + 1)} name'].get())\n",
    "\n",
    "        for i in range(int(self.layer_number_entry.get())):\n",
    "            self.x_pos_saved.append(self.x_pos[f'Layer {str(i + 1)} name'].get())\n",
    "\n",
    "        for i in range(int(self.layer_number_entry.get())):\n",
    "            self.y_pos_saved.append(self.y_pos[f'Layer {str(i + 1)} name'].get())\n",
    "\n",
    "    def draw_X(self):\n",
    "        %run draw.py\n",
    "\n",
    "        with open(\"curve_points.json\", \"r\") as file:\n",
    "            points = json.load(file)\n",
    "\n",
    "        slice_x = np.array(points)[-1]\n",
    "        self.x_points.append(slice_x)\n",
    "\n",
    "        fig = plt.Figure(figsize=(5, 4), dpi=100)\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        for i in range(len(self.x_points)):\n",
    "            ax.plot(self.x_points[i][:, 0], self.x_points[i][:, 1], \"-\")\n",
    "        ax.set_xlim(0, 800)\n",
    "        ax.set_ylim(0, 600)\n",
    "        ax.grid()\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        canvas = FigureCanvasTkAgg(fig, master=self.draw_frame)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().grid(row=int(self.layer_number_entry.get()) + 10, column=0, rowspan=1, columnspan=5, padx=10, pady=10)\n",
    "\n",
    "    def draw_Y(self):\n",
    "        %run draw.py\n",
    "\n",
    "        with open(\"curve_points.json\", \"r\") as file:\n",
    "            points = json.load(file)\n",
    "        slice_y = np.array(points)[-1]\n",
    "        self.y_points.append(slice_y)\n",
    "\n",
    "        fig = plt.Figure(figsize=(5, 4), dpi=100)\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        for i in range(len(self.y_points)):\n",
    "            ax.plot(self.y_points[i][:, 0], self.y_points[i][:, 1], \"-\")\n",
    "        ax.set_xlim(0, 800)\n",
    "        ax.set_ylim(0, 600)\n",
    "        ax.grid()\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        canvas = FigureCanvasTkAgg(fig, master=self.draw_frame)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().grid(row=int(self.layer_number_entry.get()) + 10, column=5, rowspan=1, columnspan=5, padx=10, pady=10)\n",
    "\n",
    "    def set_draw(self):\n",
    "        for i in range(int(self.layer_number_entry.get())):\n",
    "            ttk.Button(self.draw_frame, text=\"Please draw Layer \" + str(i + 1) + \" X cross-section\", style='TButton', command=self.draw_X).grid(row=i, column=0, padx=10, pady=5, sticky='w')\n",
    "            ttk.Button(self.draw_frame, text=\"Please draw Layer \" + str(i + 1) + \" Y cross-section\", style='TButton', command=self.draw_Y).grid(row=i, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        ttk.Button(self.tab3, text=\"Create model!\", style='TButton', command=self.create_model).place(relx=0.6, rely=0.015, )\n",
    "\n",
    "    def surface_all(self, auto_corr):\n",
    "        surface_all = pd.DataFrame(columns=('X', 'Y', 'Z', 'formation'))\n",
    "        for i in range(np.array(self.x_points).shape[0]):\n",
    "            slice_x = np.array(self.x_points)[i]\n",
    "            slice_y = np.array(self.y_points)[i]\n",
    "            slice_x_max = np.array([slice_x[:, 0][slice_x[:, 1] == slice_x[:, 1].min()][0], slice_x[:, 1].min()])\n",
    "            slice_y_max = np.array([slice_y[:, 0][slice_y[:, 1] == slice_y[:, 1].min()][0], slice_y[:, 1].min()])\n",
    "\n",
    "            if auto_corr:\n",
    "                diff = slice_x_max - slice_y_max\n",
    "                slice_y[:, 1] += diff[1]\n",
    "\n",
    "            x_slice = [slice_x[:, 0],\n",
    "                       np.ones(len(slice_x)) * int(self.x_pos_saved[i]),\n",
    "                       -slice_x[:, 1]]\n",
    "            x_slice = pd.DataFrame((np.array(x_slice).T), columns=('X', 'Y', 'Z'))\n",
    "\n",
    "            y_slice = [np.ones(len(slice_y)) * int(self.y_pos_saved[i]),\n",
    "                       slice_y[:, 0],\n",
    "                       -slice_y[:, 1]]\n",
    "            y_slice = pd.DataFrame((np.array(y_slice).T), columns=('X', 'Y', 'Z'))\n",
    "\n",
    "            formation = pd.DataFrame(data={'formation': [self.name_saved[i]] * (len(x_slice) + len(y_slice))})\n",
    "\n",
    "            surface = pd.concat([pd.concat([x_slice, y_slice], ignore_index=True), formation], axis=1)\n",
    "\n",
    "            surface_all = pd.concat([surface_all, surface], ignore_index=True)\n",
    "        return surface_all\n",
    "\n",
    "    def cal_orientation(self, x, y):\n",
    "        x1 = y[1] - x[1]\n",
    "        y1 = y[0] - x[0]\n",
    "        return x1 / np.sqrt(x1 ** 2 + y1 ** 2), y1 / np.sqrt(x1 ** 2 + y1 ** 2)\n",
    "\n",
    "    def save_data(self):\n",
    "        data = {\n",
    "            \"name_saved\": self.name_saved,\n",
    "            \"x_pos_saved\": self.x_pos_saved,\n",
    "            \"y_pos_saved\": self.y_pos_saved,\n",
    "            \"x_points\": [points.tolist() for points in self.x_points],\n",
    "            \"y_points\": [points.tolist() for points in self.y_points]\n",
    "        }\n",
    "        file_path = filedialog.asksaveasfilename(defaultextension=\".json\", filetypes=[(\"JSON files\", \"*.json\")])\n",
    "        if file_path:\n",
    "            with open(file_path, \"w\") as file:\n",
    "                json.dump(data, file)\n",
    "\n",
    "    def load_data(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"JSON files\", \"*.json\")])\n",
    "        if file_path:\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            self.name_saved = data[\"name_saved\"]\n",
    "            self.x_pos_saved = data[\"x_pos_saved\"]\n",
    "            self.y_pos_saved = data[\"y_pos_saved\"]\n",
    "            self.x_points = [np.array(points) for points in data[\"x_points\"]]\n",
    "            self.y_points = [np.array(points) for points in data[\"y_points\"]]\n",
    "            messagebox.showinfo(\"Load Data\", \"Data loaded successfully!\")\n",
    "            self.create_model()  # Automatically create model after loading data\n",
    "\n",
    "    def save_gempy_model(self):\n",
    "        directory = filedialog.askdirectory()\n",
    "        if directory:\n",
    "            geo_model = self._create_model_logic(self.auto_corr.get())\n",
    "            sol = gp.compute_model(gempy_model=geo_model,engine_config=gp.data.GemPyEngineConfig(use_gpu=False,\n",
    "                                                            dtype='float32', backend=gp.data.AvailableBackends.PYTORCH))\n",
    "\n",
    "            # gpv = gp.plot_3d(geo_model, image=False, plotter_type='basic', show_data=0)\n",
    "            \n",
    "            surface_points_path = os.path.join(directory, 'surface_points.csv')\n",
    "            orientations_path = os.path.join(directory, 'orientations.csv')\n",
    "            \n",
    "            geo_model.surface_points_copy.df.to_csv(surface_points_path)\n",
    "            geo_model.orientations_copy.df.to_csv(orientations_path)\n",
    "            \n",
    "            for name in self.name_saved:\n",
    "                mesh_path = os.path.join(directory, f'{name}.vtp')\n",
    "                try:\n",
    "                    gpv.plot_3d(geo_model, image=False, plotter_type='basic', show_data=0).surface_poly[name].save(mesh_path)\n",
    "                except KeyError as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    messagebox.showerror(\"Save Error\", f\"Could not save {name}.vtp. Please check the surface name.\")\n",
    "                    continue\n",
    "            \n",
    "            messagebox.showinfo(\"Save Gempy Model\", \"Gempy model saved successfully!\")\n",
    "\n",
    "    def load_vtk_file(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"VTK files\", \"*.vtp\"), (\"VTK files\", \"*.vtk\")])\n",
    "        if file_path:\n",
    "            self.mesh = pv.read(file_path)\n",
    "            self.vertices_file_path.set(file_path.replace(\".vtp\", \"_points.xlsx\"))\n",
    "            self.normals_file_path.set(file_path.replace(\".vtp\", \"_normals.xlsx\"))\n",
    "            self.scalar_file_path.set(file_path.replace(\".vtp\", \"_scalar.npy\"))\n",
    "            self.vertices_file_path.set(file_path.replace(\".vtk\", \"_points.xlsx\"))\n",
    "            self.normals_file_path.set(file_path.replace(\".vtk\", \"_normals.xlsx\"))\n",
    "            self.scalar_file_path.set(file_path.replace(\".vtk\", \"_scalar.npy\"))\n",
    "            messagebox.showinfo(\"VTK Operations\", \"VTK file loaded successfully!\")\n",
    "\n",
    "    def generate_normals(self, polydata):\n",
    "        normal_generator = vtk.vtkPolyDataNormals()\n",
    "        normal_generator.SetInputData(polydata)\n",
    "        normal_generator.ComputePointNormalsOn()\n",
    "        normal_generator.ComputeCellNormalsOff()\n",
    "        normal_generator.Update()\n",
    "        return normal_generator.GetOutput()\n",
    "\n",
    "    def get_vertices_and_normals(self, mesh):\n",
    "        surface_mesh = mesh.extract_surface()\n",
    "        polydata = surface_mesh\n",
    "\n",
    "        # Generate normals if not present\n",
    "        polydata_with_normals = self.generate_normals(polydata)\n",
    "\n",
    "        # Get points (vertices)\n",
    "        points = polydata_with_normals.GetPoints()\n",
    "        vertices = []\n",
    "        for i in range(points.GetNumberOfPoints()):\n",
    "            vertices.append(points.GetPoint(i))\n",
    "\n",
    "        # Get normals\n",
    "        normals_array = polydata_with_normals.GetPointData().GetNormals()\n",
    "        normals = []\n",
    "        for i in range(normals_array.GetNumberOfTuples()):\n",
    "            normals.append(normals_array.GetTuple(i))\n",
    "\n",
    "        return vertices, normals\n",
    "\n",
    "    def save_to_excel(self, vertices, normals, vertices_file, normals_file):\n",
    "        # Create DataFrames\n",
    "        vertices_df = pd.DataFrame(vertices, columns=['X', 'Y', 'Z'])\n",
    "        normals_df = pd.DataFrame(normals, columns=['x', 'y', 'z'])\n",
    "\n",
    "        # Save to Excel files\n",
    "        vertices_df.to_excel(vertices_file, index=False)\n",
    "        normals_df.to_excel(normals_file, index=False)\n",
    "\n",
    "    def save_vtk_to_excel(self):\n",
    "        vertices, normals = self.get_vertices_and_normals(self.mesh)\n",
    "        self.save_to_excel(vertices, normals, self.vertices_file_path.get(), self.normals_file_path.get())\n",
    "        messagebox.showinfo(\"VTK Operations\", \"Vertices and normals saved to Excel successfully!\")\n",
    "\n",
    "    def compute_scalar_field(self):\n",
    "        scalar_field = self.vtk_to_scalar(self.mesh)\n",
    "        self.scalar_field = scalar_field  # Store the scalar field for saving later\n",
    "        self.plot_scalar_field_in_gui(scalar_field)\n",
    "\n",
    "    def vtk_to_scalar(self, mesh):\n",
    "        surface_mesh = mesh.extract_surface()\n",
    "\n",
    "        # Create a uniform grid surrounding the mesh\n",
    "        x_min, x_max, y_min, y_max, z_min, z_max = surface_mesh.bounds\n",
    "        padding = 10  # add some padding to the grid dimensions\n",
    "        grid = pv.ImageData()\n",
    "        grid.dimensions = [50, 50, 50]  # adjust grid resolution as needed\n",
    "        grid.origin = [x_min - padding, y_min - padding, z_min - padding]  # start point of the grid\n",
    "        grid.spacing = [(x_max - x_min + 2 * padding) / (grid.dimensions[0] - 1),\n",
    "                        (y_max - y_min + 2 * padding) / (grid.dimensions[1] - 1),\n",
    "                        (z_max - z_min + 2 * padding) / (grid.dimensions[2] - 1)]  # grid spacing\n",
    "\n",
    "        # Calculate the distances from each grid point to the nearest point on the surface mesh\n",
    "        distances = grid.compute_implicit_distance(surface_mesh, inplace=False)\n",
    "\n",
    "        # Assign these distances as a scalar field\n",
    "        grid.point_data['Distance'] = distances.point_data['implicit_distance']\n",
    "        scalar = np.array(grid.point_data['Distance'])\n",
    "        scalar_field = scalar.reshape(50, 50, 50)\n",
    "        return scalar_field\n",
    "\n",
    "    def plot_scalar_field_in_gui(self, scalar_field):\n",
    "        def plot():\n",
    "            plotter = pv.Plotter(notebook=False)\n",
    "            surface_mesh = self.mesh.extract_surface()\n",
    "\n",
    "            # Create a uniform grid surrounding the mesh\n",
    "            x_min, x_max, y_min, y_max, z_min, z_max = surface_mesh.bounds\n",
    "            padding = 10  # add some padding to the grid dimensions\n",
    "            grid = pv.ImageData()\n",
    "            grid.dimensions = [50, 50, 50]  # adjust grid resolution as needed\n",
    "            grid.origin = [x_min - padding, y_min - padding, z_min - padding]  # start point of the grid\n",
    "            grid.spacing = [(x_max - x_min + 2 * padding) / (grid.dimensions[0] - 1),\n",
    "                            (y_max - y_min + 2 * padding) / (grid.dimensions[1] - 1),\n",
    "                            (z_max - z_min + 2 * padding) / (grid.dimensions[2] - 1)]  # grid spacing\n",
    "\n",
    "            # Calculate the distances from each grid point to the nearest point on the surface mesh\n",
    "            distances = grid.compute_implicit_distance(surface_mesh, inplace=False)\n",
    "\n",
    "            # Assign these distances as a scalar field\n",
    "            grid.point_data['Distance'] = distances.point_data['implicit_distance']\n",
    "\n",
    "            plotter.add_mesh(surface_mesh, color='white', label='Mesh Surface')\n",
    "            plotter.add_mesh(grid, scalars='Distance', cmap='viridis', opacity=0.7, label='Scalar Field')\n",
    "            plotter.add_legend()\n",
    "            plotter.show()\n",
    "\n",
    "        plot_thread = threading.Thread(target=plot)\n",
    "        plot_thread.start()\n",
    "\n",
    "    def save_scalar_field(self):\n",
    "        file_path = self.scalar_file_path.get()\n",
    "        if file_path:\n",
    "            np.save(file_path,self.vtk_to_scalar(self.mesh))\n",
    "            messagebox.showinfo(\"Save Scalar Field\", \"Scalar field saved successfully!\")\n",
    "\n",
    "    def create_model(self):\n",
    "        geo_model = self._create_model_logic(self.auto_corr.get())\n",
    "        sol = gp.compute_model(geo_model)\n",
    "        self._plot_2d(geo_model)\n",
    "\n",
    "    def plot_3d(self):\n",
    "        geo_model = self._create_model_logic(self.auto_corr.get())\n",
    "        sol = gp.compute_model(geo_model)\n",
    "        self._plot_3d(geo_model)\n",
    "\n",
    "    def _create_model_logic(self, auto_corr):\n",
    "        # Initialize a list to store orientation data for each slice\n",
    "        orientation_data = []\n",
    "        # Iterate over slices\n",
    "        self.x_points = np.array(self.x_points, dtype=object)\n",
    "        self.y_points = np.array(self.y_points, dtype=object)\n",
    "\n",
    "        for n in range(np.array(self.x_points).shape[0]):\n",
    "            slice_x = np.array(self.x_points)[n]\n",
    "            slice_y = np.array(self.y_points)[n]\n",
    "\n",
    "            # Calculate the max points in x and y slices\n",
    "            slice_x_max = np.array([slice_x[:, 0][slice_x[:, 1] == slice_x[:, 1].min()][0], slice_x[:, 1].min()])\n",
    "            slice_y_max = np.array([slice_y[:, 0][slice_y[:, 1] == slice_y[:, 1].min()][0], slice_y[:, 1].min()])\n",
    "\n",
    "            # Apply correction if auto_corr is True\n",
    "            if auto_corr:\n",
    "                diff = slice_x_max - slice_y_max\n",
    "                slice_y[:, 1] += diff[1]\n",
    "\n",
    "            # Generate pairs of indices for slice_x and slice_y\n",
    "            array_x = list(range(slice_x.shape[0]))\n",
    "            result_x = [(array_x[i], array_x[i + 1]) for i in range(len(array_x) - 1)]\n",
    "            array_y = list(range(slice_y.shape[0]))\n",
    "            result_y = [(array_y[i], array_y[i + 1]) for i in range(len(array_y) - 1)]\n",
    "\n",
    "            # Calculate orientations for slice_x\n",
    "            for i in result_x:\n",
    "                # Calculate the pole vector\n",
    "                pole_x = [self.cal_orientation(slice_x[i[0]], slice_x[i[1]])[0], 0,\n",
    "                self.cal_orientation(slice_x[i[0]], slice_x[i[1]])[1]]\n",
    "                    \n",
    "                # Append orientation data as a dictionary\n",
    "                orientation_data.append({\n",
    "                'x': slice_x[i[0]][0],\n",
    "                'y': int(self.x_pos_saved[n]),\n",
    "                'z': -slice_x[i[0]][1],\n",
    "                'surface': self.name_saved[n],\n",
    "                'G_x': pole_x[0],\n",
    "                'G_y': pole_x[1],\n",
    "                    'G_z': pole_x[2]\n",
    "                    })\n",
    "\n",
    "            # Calculate orientations for slice_y\n",
    "            for i in result_y:\n",
    "                # Calculate the pole vector\n",
    "                pole_y = [0, self.cal_orientation(slice_y[i[0]], slice_y[i[1]])[0],\n",
    "                        self.cal_orientation(slice_y[i[0]], slice_y[i[1]])[1]]\n",
    "                \n",
    "                # Append orientation data as a dictionary\n",
    "                orientation_data.append({\n",
    "                    'x': int(self.y_pos_saved[n]),\n",
    "                    'y': slice_y[i[0]][0],\n",
    "                    'z': -slice_y[i[0]][1],\n",
    "                    'surface': self.name_saved[n],\n",
    "                    'G_x': pole_y[0],\n",
    "                    'G_y': pole_y[1],\n",
    "                    'G_z': pole_y[2]\n",
    "                })\n",
    "\n",
    "        # Create DataFrame from orientation data\n",
    "        orientations_df = pd.DataFrame(orientation_data)\n",
    "        orientations_df.to_csv(\"Orientations.csv\")\n",
    "        surface_points = self.surface_all(auto_corr)\n",
    "        surface_points.to_csv(\"Surface_points.csv\")\n",
    "        geo_model: gp.data.GeoModel = gp.create_geomodel(\n",
    "        project_name='Salt',\n",
    "        extent=[0, 800, 0, 800, -600, 0],\n",
    "        resolution = [50, 50, 50],  # * Here we define the number of octree levels. If octree levels are defined, the resolution is ignored.\n",
    "        importer_helper=gp.data.ImporterHelper(\n",
    "        path_to_orientations=\"Orientations.csv\",\n",
    "        path_to_surface_points=\"Surface_points.csv\",\n",
    "        )\n",
    "        )\n",
    "\n",
    "        return geo_model\n",
    "\n",
    "    def _plot_2d(self, geo_model):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        for ax in gpv.plot_2d(geo_model, direction='x', show_data=True).axes:\n",
    "            fig_x = ax.get_figure()\n",
    "            fig_x.canvas.draw()\n",
    "            axes[0].imshow(np.array(fig_x.canvas.renderer.buffer_rgba()))\n",
    "            axes[0].axis('off')\n",
    "\n",
    "        for ax in gpv.plot_2d(geo_model, direction='y', show_data=True).axes:\n",
    "            fig_y = ax.get_figure()\n",
    "            fig_y.canvas.draw()\n",
    "            axes[1].imshow(np.array(fig_y.canvas.renderer.buffer_rgba()))\n",
    "            axes[1].axis('off')\n",
    "\n",
    "        canvas = FigureCanvasTkAgg(fig, master=self.plot_2d_frame)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().pack()\n",
    "\n",
    "    def _plot_3d(self, geo_model):\n",
    "        frame = tk.Frame(self.plot_2d_frame, width=800, height=600)\n",
    "        frame.pack(padx=10, pady=10)\n",
    "        gpv_3d = gpv.plot_3d(model=geo_model,show_surfaces=False,show_data=True,show_lith=False,image=False)\n",
    "\n",
    "    # Methods for the RBF Interpolation Tab\n",
    "    def load_surface_points(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "        if file_path:\n",
    "            self.vertices_df = pd.read_excel(file_path)\n",
    "            messagebox.showinfo(\"RBF Interpolation\", \"Surface points loaded successfully!\")\n",
    "        self.model_name = file_path.split('/')[-1].split('_')[0]\n",
    "\n",
    "    def load_orientation_points(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "        if file_path:\n",
    "            self.normals_df = pd.read_excel(file_path)\n",
    "            messagebox.showinfo(\"RBF Interpolation\", \"Orientation points loaded successfully!\")\n",
    "\n",
    "    def load_orientations(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "        if file_path:\n",
    "            self.normals_o_df = pd.read_excel(file_path)\n",
    "            messagebox.showinfo(\"RBF Interpolation\", \"Orientations loaded successfully!\")\n",
    "\n",
    "    def load_cv_surface_points(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "        if file_path:\n",
    "            self.cv_vertices_df = pd.read_excel(file_path)\n",
    "            messagebox.showinfo(\"RBF Interpolation\", \"Surface points loaded successfully!\")\n",
    "\n",
    "    def load_cv_orientation_points(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "        if file_path:\n",
    "            self.cv_normals_df = pd.read_excel(file_path)\n",
    "            messagebox.showinfo(\"RBF Interpolation\", \"Orientation points loaded successfully!\")\n",
    "\n",
    "    def create_cross_validation_tab(self):\n",
    "            # Tab 7: Cross-Validation\n",
    "        self.cv_frame = tk.Frame(self.tab7)\n",
    "        self.cv_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_surface_cv_button = ttk.Button(self.cv_frame, text=\"Load Surface Points\", style='TButton', command=self.load_cv_surface_points)\n",
    "        self.load_surface_cv_button.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_orientation_cv_button = ttk.Button(self.cv_frame, text=\"Load Orientation Points\", style='TButton', command=self.load_cv_orientation_points)\n",
    "        self.load_orientation_cv_button.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Sampling method:\", font=self.label_font).grid(row=0, column=2, padx=10, pady=5, sticky='w')\n",
    "        self.Sampling_method_cv = ttk.Combobox(self.cv_frame, font=self.entry_font, values=[\n",
    "            'Random','Systematic' ,'Statistic(K-means)',])\n",
    "        self.Sampling_method_cv.grid(row=0, column=3, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Number of Sample Surface Points:\", font=self.label_font).grid(row=1, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.num_surface_points_cv = tk.Entry(self.cv_frame, font=self.entry_font)\n",
    "        self.num_surface_points_cv.grid(row=1, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Number of Sample Orientation Points:\", font=self.label_font).grid(row=2, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.num_orientation_points_cv = tk.Entry(self.cv_frame, font=self.entry_font)\n",
    "        self.num_orientation_points_cv.grid(row=2, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Kernel Function:\", font=self.label_font).grid(row=3, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.kernel_function_cv = ttk.Combobox(self.cv_frame, font=self.entry_font, values=[\n",
    "            'cubic', 'gaussian', 'quintic', 'cubic_covariance', 'multiquadric', 'linear', \n",
    "            'thin_plate', 'inverse_quadratic', 'inverse_multiquadric', 'gaussian_covariance', \n",
    "            'ga_cub_cov', 'ga_cub'])\n",
    "        self.kernel_function_cv.grid(row=3, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Drift Type:\", font=self.label_font).grid(row=4, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.drift_type_cv = ttk.Combobox(self.cv_frame, font=self.entry_font, values=[\n",
    "            'None', 'First Order Polynomial','Second Order Polynomial', 'Ellipsoid', 'Dome Shaped'])\n",
    "        self.drift_type_cv.grid(row=4, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"eps range (start, stop, step):\", font=self.label_font).grid(row=5, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.epsilon_start_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.epsilon_start_cv.place(relx=0.63, rely=0.36, anchor='center')\n",
    "        self.epsilon_stop_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.epsilon_stop_cv.place(relx=0.73, rely=0.36, anchor='center')\n",
    "        self.epsilon_step_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.epsilon_step_cv.place(relx=0.83, rely=0.36, anchor='center')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Range value range (start, stop, step):\", font=self.label_font).grid(row=6, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.range_start_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.range_start_cv.place(relx=0.63, rely=0.42, anchor='center')\n",
    "        self.range_stop_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.range_stop_cv.place(relx=0.73, rely=0.42, anchor='center')\n",
    "        self.range_step_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.range_step_cv.place(relx=0.83, rely=0.42, anchor='center')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Parameter a range (start, stop, step):\", font=self.label_font).grid(row=7, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.param_a_start_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_a_start_cv.place(relx=0.63, rely=0.48, anchor='center')\n",
    "        self.param_a_stop_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_a_stop_cv.place(relx=0.73, rely=0.48, anchor='center')\n",
    "        self.param_a_step_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_a_step_cv.place(relx=0.83, rely=0.48, anchor='center')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Parameter b range (start, stop, step):\", font=self.label_font).grid(row=8, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.param_b_start_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_b_start_cv.place(relx=0.63, rely=0.54, anchor='center')\n",
    "        self.param_b_stop_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_b_stop_cv.place(relx=0.73, rely=0.54, anchor='center')\n",
    "        self.param_b_step_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_b_step_cv.place(relx=0.83, rely=0.54, anchor='center')\n",
    "\n",
    "        tk.Label(self.cv_frame, text=\"Parameter c range (start, stop, step):\", font=self.label_font).grid(row=9, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.param_c_start_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_c_start_cv.place(relx=0.63, rely=0.60, anchor='center')\n",
    "        self.param_c_stop_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_c_stop_cv.place(relx=0.73, rely=0.60, anchor='center')\n",
    "        self.param_c_step_cv = tk.Entry(self.cv_frame, font=self.entry_font, width=5)\n",
    "        self.param_c_step_cv.place(relx=0.83, rely=0.60, anchor='center')\n",
    "\n",
    "        self.compute_cv_button = ttk.Button(self.cv_frame, text=\"Compute Cross-Validation\", style='TButton', command=self.run_cross_validation)\n",
    "        self.compute_cv_button.grid(row=10, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.progress = ttk.Progressbar(self.cv_frame, orient=tk.HORIZONTAL, length=300, mode='determinate')\n",
    "        self.progress.grid(row=12, column=0, columnspan=4, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.elapsed_time_label = tk.Label(self.cv_frame, text=\"\", font=self.label_font)\n",
    "        self.elapsed_time_label.grid(row=13, column=0, columnspan=4, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.remaining_time_label = tk.Label(self.cv_frame, text=\"\", font=self.label_font)\n",
    "        self.remaining_time_label.grid(row=14, column=0, columnspan=4, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.result_label = tk.Label(self.cv_frame, text=\"\", font=self.label_font)\n",
    "        self.result_label.grid(row=11, column=0, columnspan=4, padx=10, pady=10, sticky='w')\n",
    "\n",
    "    def create_scalar_comparison_tab(self):\n",
    "        # Tab 7: Scalar Field Comparison\n",
    "        self.scalar_comparison_frame = tk.Frame(self.tab8)\n",
    "        self.scalar_comparison_frame.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_scalar1_button = ttk.Button(self.scalar_comparison_frame, text=\"Load Scalar Field 1\", style='TButton', command=self.load_scalar1)\n",
    "        self.load_scalar1_button.grid(row=0, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_scalar2_button = ttk.Button(self.scalar_comparison_frame, text=\"Load Scalar Field 2\", style='TButton', command=self.load_scalar2)\n",
    "        self.load_scalar2_button.grid(row=0, column=1, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_scalar3_button = ttk.Button(self.scalar_comparison_frame, text=\"Load Scalar Field 3\", style='TButton', command=self.load_scalar3)\n",
    "        self.load_scalar3_button.grid(row=0, column=2, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_scalar4_button = ttk.Button(self.scalar_comparison_frame, text=\"Load Scalar Field 4\", style='TButton', command=self.load_scalar4)\n",
    "        self.load_scalar4_button.grid(row=0, column=3, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.load_scalar5_button = ttk.Button(self.scalar_comparison_frame, text=\"Load Scalar Field 5\", style='TButton', command=self.load_scalar5)\n",
    "        self.load_scalar5_button.grid(row=0, column=4, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        tk.Label(self.scalar_comparison_frame, text=\"Compare Plane:\", font=self.label_font).grid(row=1, column=0, padx=10, pady=5, sticky='w')\n",
    "        self.compare_plane = ttk.Combobox(self.scalar_comparison_frame, font=self.entry_font, values=['x', 'y', 'z'])\n",
    "        self.compare_plane.grid(row=1, column=1, padx=10, pady=5, sticky='w')\n",
    "        self.compare_plane.current(1)  \n",
    "\n",
    "        self.flip_checkbox = tk.Checkbutton(self.scalar_comparison_frame, text=\"Flip Scalar Field 1\", variable=self.use_flip, font=self.label_font)\n",
    "        self.flip_checkbox.grid(row=2, column=0, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        self.reverse_checkbox = tk.Checkbutton(self.scalar_comparison_frame, text=\"Reverse Scalar Field 1\", variable=self.use_reverse, font=self.label_font)\n",
    "        self.reverse_checkbox.grid(row=2, column=1, padx=10, pady=5, sticky='w')\n",
    "\n",
    "        self.compare_button = ttk.Button(self.scalar_comparison_frame, text=\"Compare\", style='TButton', command=self.compare_scalars)\n",
    "        self.compare_button.grid(row=3, column=0, padx=10, pady=10, sticky='w')\n",
    "\n",
    "        self.scalar_canvas_frame = tk.Frame(self.tab8)\n",
    "        self.scalar_canvas_frame.grid(row=4, column=0, padx=10, pady=10, columnspan=4)\n",
    "\n",
    "\n",
    "    def compare_scalars(self):\n",
    "        if self.scalar1 is None or self.scalar2 is None:\n",
    "            messagebox.showerror(\"Error\", \"Please load both scalar fields before comparison.\")\n",
    "            return\n",
    "\n",
    "        scalar1 = self.scalar1\n",
    "        scalar2 = self.scalar2\n",
    "\n",
    "        scalar3 = getattr(self, 'scalar3', None)\n",
    "        scalar4 = getattr(self, 'scalar4', None)\n",
    "        scalar5 = getattr(self, 'scalar5', None)\n",
    "\n",
    "        flipped_field1 = np.zeros_like(scalar1)\n",
    "        if self.use_flip.get():\n",
    "            for i in range(scalar1.shape[1]):  \n",
    "                flipped_field1[:, i, :] = np.fliplr(scalar1[:, i, :])\n",
    "                scalar1 = flipped_field1\n",
    "\n",
    "        if self.use_reverse.get():\n",
    "            scalar1 = -scalar1\n",
    "\n",
    "        axis = self.compare_plane.get()\n",
    "        mid_index = {\n",
    "            'x': scalar1.shape[0] // 2,\n",
    "            'y': scalar1.shape[1] // 2,\n",
    "            'z': scalar1.shape[2] // 2\n",
    "        }[axis]\n",
    "\n",
    "        self.scalar_comparison(scalar1, scalar2,scalar3, scalar4,scalar5, axis, mid_index)\n",
    "\n",
    "    def scalar_comparison(self, scalar1, scalar2,scalar3=None, scalar4=None,scalar5=None, axis=None, index=None):\n",
    "        def plot_two_gradient_fields(field1, field2, diff, axis, index, title, skip=5):\n",
    "            grad_field1 = np.gradient(field1)\n",
    "            grad_field2 = np.gradient(field2)\n",
    "            if axis == 'x':\n",
    "                grad_slice1 = [grad_field1[0][index, :, :], grad_field1[1][index, :, :], grad_field1[2][index, :, :]]\n",
    "                grad_slice2 = [grad_field2[0][index, :, :], grad_field2[1][index, :, :], grad_field2[2][index, :, :]]\n",
    "                diff_slice = diff[index, :, :]\n",
    "            elif axis == 'y':\n",
    "                grad_slice1 = [grad_field1[0][:, index, :], grad_field1[1][:, index, :], grad_field1[2][:, index, :]]\n",
    "                grad_slice2 = [grad_field2[0][:, index, :], grad_field2[1][:, index, :], grad_field2[2][:, index, :]]\n",
    "                diff_slice = diff[:, index, :]\n",
    "            elif axis == 'z':\n",
    "                grad_slice1 = [grad_field1[0][:, :, index], grad_field1[1][:, :, index], grad_field1[2][:, :, index]]\n",
    "                grad_slice2 = [grad_field2[0][:, :, index], grad_field2[1][:, :, index], grad_field2[2][:, :, index]]\n",
    "                diff_slice = diff[:, :, index]\n",
    "            else:\n",
    "                raise ValueError(\"Axis must be 'x', 'y', or 'z'\")\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(3, 2.5))  \n",
    "\n",
    "            grad_x_1 = grad_slice1[0][::skip, ::skip]\n",
    "            grad_y_1 = grad_slice1[1][::skip, ::skip]\n",
    "            magnitude1 = np.sqrt(grad_x_1**2 + grad_y_1**2)\n",
    "\n",
    "            grad_x_2 = grad_slice2[0][::skip, ::skip]\n",
    "            grad_y_2 = grad_slice2[1][::skip, ::skip]\n",
    "            magnitude2 = np.sqrt(grad_x_2**2 + grad_y_2**2)\n",
    "\n",
    "            epsilon = 1e-10\n",
    "            grad_x_normalized_1 = grad_x_1 / (magnitude1 + epsilon)\n",
    "            grad_y_normalized_1 = grad_y_1 / (magnitude1 + epsilon)\n",
    "            grad_x_normalized_2 = grad_x_2 / (magnitude2 + epsilon)\n",
    "            grad_y_normalized_2 = grad_y_2 / (magnitude2 + epsilon)\n",
    "            fixed_arrow_length = 6\n",
    "\n",
    "\n",
    "\n",
    "            contour = ax.contourf(diff_slice, levels=30, cmap='cmc.grayC_r')\n",
    "            X, Y = np.meshgrid(np.arange(0, field1.shape[1], skip), np.arange(0, field1.shape[2], skip))\n",
    "            ax.quiver(X, Y, grad_x_normalized_1 * fixed_arrow_length, grad_y_normalized_1 * fixed_arrow_length,\n",
    "                        angles='xy', scale_units='xy', scale=1, color=\"orange\", width=0.003, label='Synthetic Model')\n",
    "            ax.quiver(X, Y, grad_x_normalized_2 * fixed_arrow_length, grad_y_normalized_2 * fixed_arrow_length,\n",
    "                        angles='xy', scale_units='xy', scale=1, color=\"blue\", width=0.003, label='Interpolated Model')\n",
    "            legend = plt.legend(frameon=True,fontsize=8)\n",
    "            frame = legend.get_frame()\n",
    "            frame.set_edgecolor('black')\n",
    "            ax.set_title(f'{title} (Slice {axis}={index})', fontsize=8)\n",
    "            ax.set_xticklabels([])  \n",
    "            ax.set_yticklabels([]) \n",
    "\n",
    "            return fig\n",
    "\n",
    "        def plot_scalar_field_slice(field, axis, index, title):\n",
    "            if axis == 'x':\n",
    "                slice_data = field[index, :, :]\n",
    "            elif axis == 'y':\n",
    "                slice_data = field[:, index, :]\n",
    "            elif axis == 'z':\n",
    "                slice_data = field[:, :, index]\n",
    "            else:\n",
    "                raise ValueError(\"Axis must be 'x', 'y', or 'z'\")\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(3, 2.5))  \n",
    "            contourf = ax.contourf(slice_data, levels=20, cmap='cmc.batlow')\n",
    "            contour = ax.contour(slice_data, levels=[float(0)], cmap='gray')\n",
    "            ax.set_title(f'{title} (Slice {axis}={index})', fontsize=8)\n",
    "            ax.set_xticklabels([]) \n",
    "            ax.set_yticklabels([])  \n",
    "            return fig\n",
    "\n",
    "        f_A = scalar1\n",
    "        f_B = scalar2\n",
    "        f_C = scalar3 if scalar3 is not None else None\n",
    "        f_D = scalar4 if scalar4 is not None else None\n",
    "        f_E = scalar5 if scalar5 is not None else None\n",
    "\n",
    "        grad_f_A = np.gradient(f_A)\n",
    "        grad_f_B = np.gradient(f_B)\n",
    "        grad_f_C = np.gradient(f_C) if f_C is not None else None\n",
    "        grad_f_D = np.gradient(f_D) if f_D is not None else None\n",
    "        grad_f_E = np.gradient(f_E) if f_E is not None else None\n",
    "\n",
    "        dot_product_AB = np.zeros(f_A.shape)\n",
    "        dot_product_AC = np.zeros(f_A.shape) if f_C is not None else None\n",
    "        dot_product_AD = np.zeros(f_A.shape) if f_D is not None else None\n",
    "        dot_product_AE = np.zeros(f_A.shape) if f_E is not None else None\n",
    "        magnitude_A = np.zeros(f_A.shape)\n",
    "        magnitude_B = np.zeros(f_A.shape)\n",
    "        magnitude_C = np.zeros(f_A.shape) if f_C is not None else None\n",
    "        magnitude_D = np.zeros(f_A.shape) if f_D is not None else None\n",
    "        magnitude_E = np.zeros(f_A.shape) if f_E is not None else None\n",
    "\n",
    "        for i in range(3):\n",
    "            dot_product_AB += grad_f_A[i] * grad_f_B[i]\n",
    "            magnitude_A += grad_f_A[i] ** 2\n",
    "            magnitude_B += grad_f_B[i] ** 2\n",
    "            if f_C is not None:\n",
    "                dot_product_AC += grad_f_A[i] * grad_f_C[i]\n",
    "                magnitude_C += grad_f_C[i] ** 2 \n",
    "            if f_D is not None:\n",
    "                dot_product_AD += grad_f_A[i] * grad_f_D[i]\n",
    "                magnitude_D += grad_f_D[i] ** 2\n",
    "            if f_E is not None:\n",
    "                dot_product_AE += grad_f_A[i] * grad_f_E[i]\n",
    "                magnitude_E += grad_f_E[i] ** 2\n",
    "\n",
    "\n",
    "        magnitude_A = np.sqrt(magnitude_A)\n",
    "        magnitude_B = np.sqrt(magnitude_B)\n",
    "        magnitude_C = np.sqrt(magnitude_C) if f_C is not None else None\n",
    "        magnitude_D = np.sqrt(magnitude_D) if f_D is not None else None\n",
    "        magnitude_E = np.sqrt(magnitude_E) if f_E is not None else None\n",
    "\n",
    "        epsilon = 1e-10\n",
    "        magnitude_A = np.where(magnitude_A == 0, epsilon, magnitude_A)\n",
    "        magnitude_B = np.where(magnitude_B == 0, epsilon, magnitude_B)\n",
    "        cosine_similarity_AB = dot_product_AB / (magnitude_A * magnitude_B)\n",
    "        d_AB_nabla = 1 - cosine_similarity_AB\n",
    "        if f_C is not None:\n",
    "            magnitude_C = np.where(magnitude_C == 0, epsilon, magnitude_C)\n",
    "            cosine_similarity_AC = dot_product_AC / (magnitude_A * magnitude_C)\n",
    "            d_AC_nabla = 1 - cosine_similarity_AC\n",
    "        if f_D is not None:\n",
    "            magnitude_D = np.where(magnitude_D == 0, epsilon, magnitude_D)\n",
    "            cosine_similarity_AD = dot_product_AD / (magnitude_A * magnitude_D)\n",
    "            d_AD_nabla = 1 - cosine_similarity_AD\n",
    "        if f_E is not None:\n",
    "            magnitude_E = np.where(magnitude_E == 0, epsilon, magnitude_E)\n",
    "            cosine_similarity_AE = dot_product_AE / (magnitude_A * magnitude_E)\n",
    "            d_AE_nabla = 1 - cosine_similarity_AE\n",
    "\n",
    "        figA = plot_scalar_field_slice(f_A, axis, index, 'Synthetic Scalar Field', )\n",
    "        figB = plot_scalar_field_slice(f_B, axis, index, 'Interpolated Scalar Field', ) \n",
    "        figAB = plot_two_gradient_fields(f_A, f_B, d_AB_nabla, axis, index, 'Difference plot', skip=7)\n",
    "        if f_C is not None:\n",
    "            figC = plot_scalar_field_slice(f_C, axis, index, 'Interpolated Scalar Field', )\n",
    "            figAC = plot_two_gradient_fields(f_A, f_C, d_AC_nabla, axis, index, 'Difference plot', skip=7)\n",
    "        if f_D is not None:\n",
    "            figD = plot_scalar_field_slice(f_D, axis, index, 'Interpolated Scalar Field', )\n",
    "            figAD = plot_two_gradient_fields(f_A, f_D, d_AD_nabla, axis, index, 'Difference plot', skip=7)\n",
    "        if f_E is not None:\n",
    "            figE = plot_scalar_field_slice(f_E, axis, index, 'Interpolated Scalar Field', )                                      \n",
    "            figAE = plot_two_gradient_fields(f_A, f_E, d_AE_nabla, axis, index, 'Difference plot', skip=7)\n",
    "\n",
    "        mean_diff_AB = np.mean(d_AB_nabla)\n",
    "        rms_diff_AB = np.sqrt(np.mean(d_AB_nabla**2))\n",
    "        max_diff_AB = np.max(np.abs(d_AB_nabla))\n",
    "        median_diff_AB = np.median(d_AB_nabla)\n",
    "\n",
    "        if f_C is not None:\n",
    "            mean_diff_AC = np.mean(d_AC_nabla)\n",
    "            rms_diff_AC = np.sqrt(np.mean(d_AC_nabla**2))\n",
    "            max_diff_AC = np.max(np.abs(d_AC_nabla))\n",
    "            median_diff_AC = np.median(d_AC_nabla)\n",
    "        if f_D is not None:\n",
    "            mean_diff_AD = np.mean(d_AD_nabla)\n",
    "            rms_diff_AD = np.sqrt(np.mean(d_AD_nabla**2))\n",
    "            max_diff_AD = np.max(np.abs(d_AD_nabla))\n",
    "            median_diff_AD = np.median(d_AD_nabla)\n",
    "        if f_E is not None:\n",
    "            mean_diff_AE = np.mean(d_AE_nabla)\n",
    "            rms_diff_AE = np.sqrt(np.mean(d_AE_nabla**2))\n",
    "            max_diff_AE = np.max(np.abs(d_AE_nabla))\n",
    "            median_diff_AE = np.median(d_AE_nabla)\n",
    "\n",
    "        for widget in self.scalar_canvas_frame.winfo_children():\n",
    "            widget.destroy()\n",
    "\n",
    "        stats_frame = tk.Frame(self.scalar_canvas_frame)\n",
    "        stats_frame.grid(row=0, column=1, columnspan=6, pady=5)\n",
    "\n",
    "        tk.Label(stats_frame, text=f\"Mean Difference: {mean_diff_AB:.3f}\", font=self.label_font_s).grid(row=0, column=2, sticky='w')\n",
    "        tk.Label(stats_frame, text=f\"RMS Difference: {rms_diff_AB:.3f}\", font=self.label_font_s).grid(row=0, column=3, sticky='w')\n",
    "        tk.Label(stats_frame, text=f\"Maximum Difference: {max_diff_AB:.3f}\", font=self.label_font_s).grid(row=1, column=2, sticky='w')\n",
    "        tk.Label(stats_frame, text=f\"Median Difference: {median_diff_AB:.3f}\", font=self.label_font_s).grid(row=1, column=3, sticky='w')\n",
    "\n",
    "        if f_C is not None:\n",
    "            tk.Label(stats_frame, text=f\"Mean Difference: {mean_diff_AC:.3f}\", font=self.label_font_s).grid(row=0, column=4, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"RMS Difference: {rms_diff_AC:.3f}\", font=self.label_font_s).grid(row=0, column=5, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"Maximum Difference: {max_diff_AC:.3f}\", font=self.label_font_s).grid(row=1, column=4, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"Median Difference: {median_diff_AC:.3f}\", font=self.label_font_s).grid(row=1, column=5, sticky='w')\n",
    "        \n",
    "        if f_D is not None:\n",
    "            tk.Label(stats_frame, text=f\"Mean Difference: {mean_diff_AD:.3f}\", font=self.label_font_s).grid(row=0, column=6, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"RMS Difference: {rms_diff_AD:.3f}\", font=self.label_font_s).grid(row=0, column=7, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"Maximum Difference: {max_diff_AD:.3f}\", font=self.label_font_s).grid(row=1, column=6, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"Median Difference: {median_diff_AD:.3f}\", font=self.label_font_s).grid(row=1, column=7, sticky='w')\n",
    "\n",
    "        if f_E is not None:\n",
    "            tk.Label(stats_frame, text=f\"Mean Difference: {mean_diff_AE:.3f}\", font=self.label_font_s).grid(row=0, column=8, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"RMS Difference: {rms_diff_AE:.3f}\", font=self.label_font_s).grid(row=0, column=9, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"Maximum Difference: {max_diff_AE:.3f}\", font=self.label_font_s).grid(row=1, column=8, sticky='w')\n",
    "            tk.Label(stats_frame, text=f\"Median Difference: {median_diff_AE:.3f}\", font=self.label_font_s).grid(row=1, column=9, sticky='w')\n",
    "\n",
    "        canvas1 = FigureCanvasTkAgg(figA, master=self.scalar_canvas_frame)\n",
    "        canvas1.draw()\n",
    "        canvas1.get_tk_widget().grid(row=1, column=0, padx=5, pady=5)\n",
    "\n",
    "        canvas2 = FigureCanvasTkAgg(figB, master=self.scalar_canvas_frame)\n",
    "        canvas2.draw()\n",
    "        canvas2.get_tk_widget().grid(row=1, column=1, padx=5, pady=5)\n",
    "\n",
    "        canvas6 = FigureCanvasTkAgg(figAB, master=self.scalar_canvas_frame)\n",
    "        canvas6.draw()\n",
    "        canvas6.get_tk_widget().grid(row=2, column=1, padx=5, pady=5)\n",
    "\n",
    "        if f_C is not None:\n",
    "            canvas3 = FigureCanvasTkAgg(figC, master=self.scalar_canvas_frame)\n",
    "            canvas3.draw()\n",
    "            canvas3.get_tk_widget().grid(row=1, column=2, padx=5, pady=5)\n",
    "            canvas7 = FigureCanvasTkAgg(figAC, master=self.scalar_canvas_frame)\n",
    "            canvas7.draw()\n",
    "            canvas7.get_tk_widget().grid(row=2, column=2, padx=5, pady=5)\n",
    "        \n",
    "        if f_D is not None:\n",
    "            canvas4 = FigureCanvasTkAgg(figD, master=self.scalar_canvas_frame)\n",
    "            canvas4.draw()\n",
    "            canvas4.get_tk_widget().grid(row=1, column=3, padx=5, pady=5)\n",
    "            canvas8 = FigureCanvasTkAgg(figAD, master=self.scalar_canvas_frame)\n",
    "            canvas8.draw()\n",
    "            canvas8.get_tk_widget().grid(row=2, column=3, padx=5, pady=5)\n",
    "\n",
    "        if f_E is not None:\n",
    "            canvas5 = FigureCanvasTkAgg(figE, master=self.scalar_canvas_frame)\n",
    "            canvas5.draw()\n",
    "            canvas5.get_tk_widget().grid(row=1, column=4, padx=5, pady=5)\n",
    "            canvas9 = FigureCanvasTkAgg(figAE, master=self.scalar_canvas_frame)\n",
    "            canvas9.draw()\n",
    "            canvas9.get_tk_widget().grid(row=2, column=4, padx=5, pady=5)\n",
    "\n",
    "    def load_scalar1(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"NumPy files\", \"*.npy\")])\n",
    "        if file_path:\n",
    "            self.scalar1 = np.load(file_path)\n",
    "            messagebox.showinfo(\"Load Scalar Field 1\", \"Scalar Field 1 loaded successfully!\")\n",
    "\n",
    "    def load_scalar2(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"NumPy files\", \"*.npy\")])\n",
    "        if file_path:\n",
    "            self.scalar2 = np.load(file_path)\n",
    "            messagebox.showinfo(\"Load Scalar Field 2\", \"Scalar Field 2 loaded successfully!\")\n",
    "\n",
    "    def load_scalar3(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"NumPy files\", \"*.npy\")])\n",
    "        if file_path:\n",
    "            self.scalar3 = np.load(file_path)\n",
    "            messagebox.showinfo(\"Load Scalar Field 3\", \"Scalar Field 3 loaded successfully!\")\n",
    "\n",
    "    def load_scalar4(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"NumPy files\", \"*.npy\")])\n",
    "        if file_path:\n",
    "            self.scalar4 = np.load(file_path)\n",
    "            messagebox.showinfo(\"Load Scalar Field 4\", \"Scalar Field 4 loaded successfully!\")\n",
    "\n",
    "    def load_scalar5(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"NumPy files\", \"*.npy\")])\n",
    "        if file_path:\n",
    "            self.scalar5 = np.load(file_path)\n",
    "            messagebox.showinfo(\"Load Scalar Field 5\", \"Scalar Field 5 loaded successfully!\")\n",
    "\n",
    "    def load_drift_scalar(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"NumPy files\", \"*.npy\")])\n",
    "        if file_path:\n",
    "            self.drift_scalar = np.load(file_path)\n",
    "            messagebox.showinfo(\"Load Drift Scalar Field\", \"Drift Scalar Field loaded successfully!\")\n",
    "\n",
    "    def RBF_3D_kernel(self, G_position, G_orientation, layer_position, test_data, eps, range_val, drift=0, cv=True, a=1, b=1, c=1):\n",
    "\n",
    "        seed = 55\n",
    "        num_surface_points = int(self.num_surface_points_cv.get())\n",
    "        num_orientation_points = int(self.num_orientation_points_cv.get())\n",
    "        func = self.kernel_function_cv.get()\n",
    "        G_1 = G_position\n",
    "        G_1_o = G_orientation\n",
    "        layer1 = layer_position\n",
    "\n",
    "        def squared_euclidean_distance(x_1, x_2):\n",
    "            sqd = np.sqrt(np.reshape(np.sum(x_1**2, 1), newshape=(x_1.shape[0], 1)) +\n",
    "                        np.reshape(np.sum(x_2**2, 1), newshape=(1, x_2.shape[0])) -\n",
    "                        2 * (x_1 @ x_2.T))\n",
    "            return np.nan_to_num(sqd)\n",
    "        dis = squared_euclidean_distance(self.systematic_sampling(self.cv_vertices_df, num_surface_points)[['X','Y','Z']].values, self.systematic_sampling(self.cv_vertices_df, num_surface_points)[['X','Y','Z']].values)\n",
    "        _eps = dis.mean()\n",
    "        _range_val = dis.max() * 2\n",
    "\n",
    "        def kernel(radius, function, a=1, b=1, n=1):\n",
    "            if function == 'cubic':\n",
    "                return radius**3\n",
    "            elif function == 'gaussian':\n",
    "                return np.exp(-1 * (radius**2)/ (2* eps**2))\n",
    "            elif function == 'quintic':\n",
    "                return radius**5\n",
    "            elif function == 'cubic_covariance':\n",
    "                return (1 - 7 * (radius / range_val) ** 2 +\n",
    "                        35 / 4 * (radius / range_val) ** 3 -\n",
    "                        7 / 2 * (radius / range_val) ** 5 +\n",
    "                        3 / 4 * (radius / range_val) ** 7)\n",
    "            elif function == 'multiquadric':\n",
    "                return np.sqrt(radius**2+eps**2)\n",
    "            elif function == 'linear':\n",
    "                return -radius\n",
    "            elif function == 'thin_plate':\n",
    "                return radius**2 * np.log(radius)\n",
    "            elif function == 'inverse_quadratic':\n",
    "                return 1 / (eps**2 + radius**2)\n",
    "            elif function == 'inverse_multiquadric':\n",
    "                return 1 / np.sqrt(radius**2+eps**2)\n",
    "            elif function == 'gaussian_covariance':\n",
    "                return np.exp(-(radius**2 / range_val**2))\n",
    "            elif function == 'ga_cub_cov':\n",
    "                return a * (1 - np.exp(-(radius / range_val)**2)) + b * (1 - 7 * (radius / range_val) ** 2 +\n",
    "                    35 / 4 * (radius / range_val) ** 3 - 7 / 2 * (radius / range_val) ** 5 +\n",
    "                    3 / 4 * (radius / range_val) ** 7)\n",
    "            elif function == 'ga_cub':\n",
    "                return np.exp(-1 * radius**2 / (2 * eps**2)) + n * (radius**3)\n",
    "\n",
    "        df = egrad(kernel)\n",
    "        ddf = egrad(df)\n",
    "\n",
    "        def squared_euclidean_distance(x_1, x_2):\n",
    "            sqd = np.sqrt(np.reshape(np.sum(x_1**2, 1), newshape=(x_1.shape[0], 1)) +\n",
    "                        np.reshape(np.sum(x_2**2, 1), newshape=(1, x_2.shape[0])) -\n",
    "                        2 * (x_1 @ x_2.T))\n",
    "            return np.nan_to_num(sqd)\n",
    "\n",
    "        def cartesian_dist(x_1, x_2):\n",
    "            return np.concatenate([\n",
    "                np.tile(x_1[:, 0] - np.reshape(x_2[:, 0], [x_2[:, 0].shape[0], 1]), [1, 3]),\n",
    "                np.tile(x_1[:, 1] - np.reshape(x_2[:, 1], [x_2[:, 1].shape[0], 1]), [1, 3]),\n",
    "                np.tile(x_1[:, 2] - np.reshape(x_2[:, 2], [x_2[:, 2].shape[0], 1]), [1, 3])], axis=0)\n",
    "\n",
    "        def cov_gradients(dist_tiled):\n",
    "            a = (h_u * h_v)\n",
    "            b = dist_tiled**2\n",
    "\n",
    "            t1 = np.divide(a, b, out=np.zeros_like(a), casting='unsafe', where=b!=0)\n",
    "\n",
    "            t2 = npy.where(dist_tiled < range_val, npy.nan_to_num(ddf(dist_tiled, function=func)) - npy.nan_to_num(df(dist_tiled, function=func)/dist_tiled), 0)\n",
    "            \n",
    "            t3 = perpendicularity_matrix * np.where(dist_tiled < range_val, (np.nan_to_num(df(dist_tiled, function=func)/dist_tiled)), 0)\n",
    "            t4 = 1/3 * np.eye(dist_tiled.shape[0])\n",
    "\n",
    "            C_G = t1 * t2 - t3 + t4\n",
    "            return C_G\n",
    "\n",
    "        def set_rest_ref_matrix(number_of_points_per_surface):\n",
    "            ref_layer_points = np.repeat(np.stack([layer1[-1]], axis=0), repeats=number_of_points_per_surface-1, axis=0)\n",
    "            rest_layer_points = np.concatenate([layer1[0:-1]], axis=0)\n",
    "            return ref_layer_points, rest_layer_points\n",
    "\n",
    "        def cov_interface(ref_layer_points, rest_layer_points):\n",
    "            sed_rest_rest = squared_euclidean_distance(rest_layer_points, rest_layer_points)\n",
    "            sed_ref_rest = squared_euclidean_distance(ref_layer_points, rest_layer_points)\n",
    "            sed_rest_ref = squared_euclidean_distance(rest_layer_points, ref_layer_points)\n",
    "            sed_ref_ref = squared_euclidean_distance(ref_layer_points, ref_layer_points)\n",
    "\n",
    "            C_I = kernel(sed_rest_rest, function=func) - kernel(sed_ref_rest, function=func) - kernel(sed_rest_ref, function=func) + kernel(sed_ref_ref, function=func)\n",
    "            return C_I\n",
    "\n",
    "        def cartesian_dist_no_tile(x_1, x_2):\n",
    "            return np.concatenate([\n",
    "                np.transpose((x_1[:, 0] - np.reshape(x_2[:, 0], [x_2.shape[0], 1]))),\n",
    "                np.transpose((x_1[:, 1] - np.reshape(x_2[:, 1], [x_2.shape[0], 1]))),\n",
    "                np.transpose((x_1[:, 2] - np.reshape(x_2[:, 2], [x_2.shape[0], 1])))\n",
    "            ], axis=0)\n",
    "\n",
    "        def cov_interface_gradients(hu_rest, hu_ref):\n",
    "            C_GI = hu_rest * np.where(sed_dips_rest < range_val, (np.nan_to_num(df(sed_dips_rest, function=func) / sed_dips_rest)), 0) - \\\n",
    "                hu_ref * np.where(sed_dips_ref < range_val, (np.nan_to_num(df(sed_dips_ref, function=func) / sed_dips_ref)), 0)\n",
    "            return C_GI\n",
    "\n",
    "        def perpendicularity(G_1):\n",
    "            a = np.concatenate([np.ones([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]])], axis=1)\n",
    "            b = np.concatenate([np.zeros([G_1.shape[0], G_1.shape[0]]), np.ones([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]])], axis=1)\n",
    "            c = np.concatenate([np.zeros([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]]), np.ones([G_1.shape[0], G_1.shape[0]])], axis=1)\n",
    "            return np.concatenate([a, b, c], axis=0)\n",
    "        \n",
    "        \n",
    "        G_1_tiled = np.tile(G_1, [3, 1])\n",
    "\n",
    "        h_u = cartesian_dist(G_1, G_1)\n",
    "        h_v = h_u.T\n",
    "\n",
    "        perpendicularity_matrix = perpendicularity(G_1)\n",
    "\n",
    "        dist_tiled = squared_euclidean_distance(G_1_tiled, G_1_tiled)\n",
    "\n",
    "        dist_tiled = dist_tiled + np.eye(dist_tiled.shape[0])\n",
    "\n",
    "        C_G = cov_gradients(dist_tiled)\n",
    "\n",
    "        number_of_points_per_surface = np.array([layer1.shape[0]])\n",
    "\n",
    "        ref_layer_points, rest_layer_points = set_rest_ref_matrix(number_of_points_per_surface)\n",
    "\n",
    "        C_I = cov_interface(ref_layer_points, rest_layer_points)\n",
    "\n",
    "        sed_dips_rest = squared_euclidean_distance(G_1_tiled, rest_layer_points)\n",
    "        sed_dips_ref = squared_euclidean_distance(G_1_tiled, ref_layer_points)\n",
    "\n",
    "        hu_rest = cartesian_dist_no_tile(G_1, rest_layer_points)\n",
    "        hu_ref = cartesian_dist_no_tile(G_1, ref_layer_points)\n",
    "\n",
    "        C_GI = cov_interface_gradients(hu_rest, hu_ref)\n",
    "        C_IG = C_GI.T\n",
    "\n",
    "        x0 = (layer1[:, 0].min() + layer1[:, 0].max()) / 2\n",
    "        y0 = (layer1[:, 1].min() + layer1[:, 1].max()) / 2\n",
    "        z0 = (layer1[:, 2].min() + layer1[:, 2].max()) / 2\n",
    "\n",
    "        xx = np.linspace(layer1[:, 0].min() - 100, layer1[:, 0].max() + 100, 50)\n",
    "        yy = np.linspace(layer1[:, 1].min() - 100, layer1[:, 1].max() + 100, 50)\n",
    "        zz = np.linspace(layer1[:, 2].min() - 100, layer1[:, 2].max() + 100, 50)\n",
    "        XX, YY, ZZ = np.meshgrid(xx, yy, zz)\n",
    "        X = np.reshape(XX, [-1]).T\n",
    "        Y = np.reshape(YY, [-1]).T\n",
    "        Z = np.reshape(ZZ, [-1]).T\n",
    "        if cv == True:\n",
    "            grid = test_data\n",
    "        else:\n",
    "            grid = np.stack([X, Y, Z], axis=1)\n",
    "\n",
    "        hu_Simpoints = cartesian_dist_no_tile(G_1, grid)\n",
    "        sed_dips_SimPoint = squared_euclidean_distance(G_1_tiled, grid)\n",
    "        sed_rest_SimPoint = squared_euclidean_distance(rest_layer_points, grid)\n",
    "        sed_ref_SimPoint = squared_euclidean_distance(ref_layer_points, grid)\n",
    "\n",
    "        if drift == 1:  # 'Second Order Polynomial'\n",
    "            K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "            n = G_1.shape[0]\n",
    "            sub_x = np.tile(np.array([[1,0,0]]),[n,1])\n",
    "            sub_y = np.tile(np.array([[0,1,0]]),[n,1])\n",
    "            sub_z = np.tile(np.array([[0,0,1]]),[n,1])\n",
    "            sub_block1 = np.concatenate([sub_x,sub_y,sub_z],0)\n",
    "\n",
    "            sub_x_2 = np.zeros((n, 3))\n",
    "            sub_y_2 = np.zeros((n, 3))\n",
    "            sub_z_2 = np.zeros((n, 3))\n",
    "            sub_x_2[:, 0] = 2 * G_1[:, 0]\n",
    "            sub_y_2[:, 1] = 2 * G_1[:, 1]\n",
    "            sub_z_2[:, 2] = 2 * G_1[:, 2]\n",
    "\n",
    "            sub_block2 = np.concatenate([sub_x_2, sub_y_2, sub_z_2], 0)\n",
    "\n",
    "            sub_xy = np.reshape(np.concatenate([G_1[:, 1],G_1[:, 0]], 0), [2 * n, 1])\n",
    "            sub_xy = np.pad(sub_xy, [[0, n], [0, 0]])\n",
    "\n",
    "            sub_xz = np.concatenate([np.pad(np.reshape(G_1[:, 2], [n, 1]), [[0, n], [0, 0]]), np.reshape(G_1[:, 0], [n, 1])], 0)\n",
    "\n",
    "            sub_yz = np.reshape(np.concatenate([G_1[:, 2], G_1[:, 1]], 0), [2 * n, 1])\n",
    "            sub_yz = np.pad(sub_yz, [[n, 0], [0, 0]])\n",
    "\n",
    "            sub_block3 = np.concatenate([sub_xy, sub_xz, sub_yz], 1)\n",
    "\n",
    "            U_G = np.concatenate([sub_block1, sub_block2, sub_block3], 1)\n",
    "            # print(U_G.shape)\n",
    "            U_I = -np.stack([(rest_layer_points[:, 0] - ref_layer_points[:, 0]), \n",
    "                            (rest_layer_points[:, 1] - ref_layer_points[:, 1]),\n",
    "                            (rest_layer_points[:, 2] - ref_layer_points[:, 2]),\n",
    "                            (rest_layer_points[:, 0] ** 2 - ref_layer_points[:, 0] ** 2),\n",
    "                            (rest_layer_points[:, 1] ** 2 - ref_layer_points[:, 1] ** 2),\n",
    "                            (rest_layer_points[:, 2] ** 2 - ref_layer_points[:, 2] ** 2),\n",
    "                            (rest_layer_points[:, 0] * rest_layer_points[:, 1] - ref_layer_points[:, 0] * ref_layer_points[:, 1]),\n",
    "                            (rest_layer_points[:, 0] * rest_layer_points[:, 2] - ref_layer_points[:, 0] * ref_layer_points[:, 2]),\n",
    "                            (rest_layer_points[:, 1] * rest_layer_points[:, 2] - ref_layer_points[:, 1] * ref_layer_points[:, 2])], 1)\n",
    "\n",
    "            length_of_CG = C_G.shape[1]\n",
    "            length_of_CGI = C_GI.shape[1]\n",
    "            U_G = U_G[:length_of_CG, :9]\n",
    "            U_I = U_I[:length_of_CGI, :9]\n",
    "\n",
    "\n",
    "            U = np.concatenate([U_G,U_I],0)\n",
    "\n",
    "            # build zero matrix\n",
    "            zero_matrix = np.zeros([U.shape[1],U.shape[1]])\n",
    "\n",
    "            # concatenate drift matrix and kriging matrix\n",
    "            K_D = np.concatenate([np.concatenate([K,U],axis = 1),np.concatenate([U.T,zero_matrix],axis = 1)],axis = 0)\n",
    "            # build right side matrix of cokriging system\n",
    "            bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "            bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "            # solve kriging weight\n",
    "            w = np.linalg.lstsq(K_D,bk)[0]\n",
    "            # gradient contribution\n",
    "            sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "            sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "            # surface point contribution\n",
    "            sigma_0_interf = -w[G_1.shape[0]*3:-9]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "            sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "            # 2nd order drift contribution\n",
    "            sigma_0_2nd_drift_1 = grid[:,0] * w[-9] + grid[:,1] * w[-8] + grid[:,2] * w[-7] + \\\n",
    "                grid[:,0] ** 2 * w[-6] + grid[:,1] ** 2 * w[-5] + grid[:,2] ** 2 * w[-4] + \\\n",
    "                grid[:,0] * grid[:,1] * w[-3] + grid[:,0] * grid[:,2] * w[-2] + grid[:,1] * grid[:,2] * w[-1]\n",
    "            sigma_0_2nd_drift = sigma_0_2nd_drift_1\n",
    "            interpolate_result = 1*sigma_0_grad+1*sigma_0_interf + 1*sigma_0_2nd_drift \n",
    "\n",
    "        elif drift == 0:  # 'None'\n",
    "            K_D = np.nan_to_num(np.concatenate([np.concatenate([C_G, C_GI], axis=1), np.concatenate([C_IG, C_I], axis=1)], axis=0))\n",
    "\n",
    "            bk = np.concatenate([G_1_o[:, 0], G_1_o[:, 1], G_1_o[:, 2], np.zeros(K_D.shape[0] - G_1.shape[0] * 3)], axis=0)\n",
    "            bk = np.reshape(bk, newshape=[bk.shape[0], 1])\n",
    "\n",
    "            w = np.linalg.lstsq(K_D, bk, rcond=None)[0]\n",
    "            sigma_0_grad = w[:G_1.shape[0] * 3] * (-hu_Simpoints * (sed_dips_SimPoint < range_val) * (np.nan_to_num(df(sed_dips_SimPoint, function=func) / sed_dips_SimPoint)))\n",
    "\n",
    "            sigma_0_grad = np.sum(sigma_0_grad, axis=0)\n",
    "            sigma_0_interf = -w[G_1.shape[0] * 3:] * (((sed_rest_SimPoint < range_val) * (kernel(sed_rest_SimPoint, function=func)) - (sed_ref_SimPoint < range_val) * (kernel(sed_ref_SimPoint, function=func))))\n",
    "            sigma_0_interf = np.sum(sigma_0_interf, axis=0)\n",
    "\n",
    "            interpolate_result = 1 * sigma_0_grad + 1 * sigma_0_interf\n",
    "\n",
    "        elif drift == 2:  # 'Spherical'\n",
    "            r = 1\n",
    "            K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "            D_Z = ((2*(G_1[:,2]-z0)/c)**2).reshape(G_1.shape[0],1)\n",
    "            D_X = ((2*(G_1[:,0]-x0)/a)**2).reshape(G_1.shape[0],1)\n",
    "            D_Y = ((2*(G_1[:,1]-y0)/b)**2).reshape(G_1.shape[0],1)\n",
    "            D_I = ((((ref_layer_points[:,0]-x0)/a)**2+((ref_layer_points[:,1]-y0)/b)**2+((ref_layer_points[:,2]-z0)/c)**2-r**2) \\\n",
    "                - (((rest_layer_points[:,0]-x0)/a)**2+((rest_layer_points[:,1]-y0)/b)**2+((rest_layer_points[:,2]-z0)/c)**2-r**2)).reshape(-1,1)       \n",
    "            # build external drift matrix\n",
    "            D_E = np.concatenate([D_X , D_Y , D_Z , D_I],axis=0)\n",
    "            D_E_T = D_E.T\n",
    "            # D = np.concatenate([D_U,D_E],axis=1)\n",
    "            D = D_E\n",
    "            D_T = D.T \n",
    "            # build zero matrix\n",
    "            zero_matrix = np.zeros([D.shape[1],D.shape[1]])\n",
    "            # concatenate drift matrix and kriging matrix\n",
    "            K_D = np.concatenate([np.concatenate([K,D],axis = 1),np.concatenate([D_T,zero_matrix],axis = 1)],axis = 0)\n",
    "\n",
    "            # build right side matrix of cokriging system\n",
    "            bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "            bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "            # solve kriging weight\n",
    "            w = np.linalg.lstsq(K_D,bk)[0]\n",
    "            # gradient contribution\n",
    "            sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "            sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "            # surface point contribution\n",
    "            sigma_0_interf = -w[G_1.shape[0]*3:-1]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "            sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "            external_drift = (((grid[:,0]-x0)/a)**2+((grid[:,1]-y0)/b)**2+((grid[:,2]-z0)/c)**2+r**2) * (w[-1]).T\n",
    "            interpolate_result = 1*sigma_0_grad+1*sigma_0_interf   + 1* external_drift\n",
    "\n",
    "\n",
    "        elif drift == 3:  # 'Dome Shaped'\n",
    "            K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "            # z - (c - (x**2 + y**2) / a**2) * np.exp(-(x**2 + y**2) / b**2)\n",
    "            def F_x(x, y, z, a, b, c):\n",
    "                return z - (c - ((x - x0)**2 + (y - y0)**2) / a**2) * np.exp(-((x - x0)**2 + (y - y0)**2) / b**2)\n",
    "            \n",
    "            def partial_F_x(x, y, a, b, c):\n",
    "                term1 = (2 * (x - x0) * (c - ((x - x0)**2 + (y - y0)**2) / a**2)) / b**2\n",
    "                term2 = (2 * (x - x0)) / a**2\n",
    "                return (term1 + term2) * np.exp(-((x - x0)**2 + (y - y0)**2) / b**2)\n",
    "\n",
    "            def partial_F_y(y, x, a, b, c):\n",
    "                term1 = (2 * (y - y0) * (c - ((x - x0)**2 + (y - y0)**2) / a**2)) / b**2\n",
    "                term2 = (2 * (y - y0)) / a**2\n",
    "                return (term1 + term2) * np.exp(-((x - x0)**2 + (y - y0)**2) / b**2)\n",
    "\n",
    "            def partial_F_z():\n",
    "                return np.ones_like(x)\n",
    "            \n",
    "            x = G_1[:, 0]\n",
    "            y = G_1[:, 1]\n",
    "            z = G_1[:, 2]\n",
    "\n",
    "            D_Z = partial_F_z().reshape(G_1.shape[0],1)\n",
    "            D_X = partial_F_x(x, y, a, b, c).reshape(G_1.shape[0],1)\n",
    "            D_Y = partial_F_y(y, x, a, b, c).reshape(G_1.shape[0],1)\n",
    "            D_I = (F_x(ref_layer_points[:,0], ref_layer_points[:,1], ref_layer_points[:,2], a, b, c) \\\n",
    "                - F_x(rest_layer_points[:,0], rest_layer_points[:,1], rest_layer_points[:,2], a, b, c)).reshape(-1,1)\n",
    "            # build external drift matrix\n",
    "            D = np.concatenate([D_X , D_Y , D_Z , D_I],axis=0)\n",
    "            D_T = D.T \n",
    "            # build zero matrix\n",
    "            zero_matrix = np.zeros([D.shape[1],D.shape[1]])\n",
    "            # concatenate drift matrix and kriging matrix\n",
    "            K_D = np.concatenate([np.concatenate([K,D],axis = 1),np.concatenate([D_T,zero_matrix],axis = 1)],axis = 0)\n",
    "            # build right side matrix of cokriging system\n",
    "            bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "            bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "            # solve kriging weight\n",
    "            w = np.linalg.lstsq(K_D,bk)[0]\n",
    "            # gradient contribution\n",
    "            sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "            sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "            # surface point contribution\n",
    "            sigma_0_interf = -w[G_1.shape[0]*3:-1]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "            sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "            external_drift = F_x(grid[:,0],grid[:,1],grid[:,2],a,b,c) * (w[-1]).T\n",
    "            interpolate_result = 1*sigma_0_grad+1*sigma_0_interf   + 1* external_drift\n",
    "\n",
    "        if drift == 4:  # 'First Order Polynomial'\n",
    "            K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "            n = G_1.shape[0]\n",
    "            sub_x = np.tile(np.array([[1,0,0]]),[n,1])\n",
    "            sub_y = np.tile(np.array([[0,1,0]]),[n,1])\n",
    "            sub_z = np.tile(np.array([[0,0,1]]),[n,1])\n",
    "            sub_block1 = np.concatenate([sub_x,sub_y,sub_z],0)\n",
    "\n",
    "            sub_x_2 = np.zeros((n, 3))\n",
    "            sub_y_2 = np.zeros((n, 3))\n",
    "            sub_z_2 = np.zeros((n, 3))\n",
    "            sub_x_2[:, 0] = 2 * G_1[:, 0]\n",
    "            sub_y_2[:, 1] = 2 * G_1[:, 1]\n",
    "            sub_z_2[:, 2] = 2 * G_1[:, 2]\n",
    "\n",
    "            sub_block2 = np.concatenate([sub_x_2, sub_y_2, sub_z_2], 0)\n",
    "\n",
    "            sub_xy = np.reshape(np.concatenate([G_1[:, 1],G_1[:, 0]], 0), [2 * n, 1])\n",
    "            sub_xy = np.pad(sub_xy, [[0, n], [0, 0]])\n",
    "\n",
    "            sub_xz = np.concatenate([np.pad(np.reshape(G_1[:, 2], [n, 1]), [[0, n], [0, 0]]), np.reshape(G_1[:, 0], [n, 1])], 0)\n",
    "\n",
    "            sub_yz = np.reshape(np.concatenate([G_1[:, 2], G_1[:, 1]], 0), [2 * n, 1])\n",
    "            sub_yz = np.pad(sub_yz, [[n, 0], [0, 0]])\n",
    "\n",
    "            sub_block3 = np.concatenate([sub_xy, sub_xz, sub_yz], 1)\n",
    "\n",
    "            U_G = np.concatenate([sub_block1, sub_block2, sub_block3], 1)\n",
    "            U_I = -np.stack([(rest_layer_points[:, 0] - ref_layer_points[:, 0]), \n",
    "                            (rest_layer_points[:, 1] - ref_layer_points[:, 1]),\n",
    "                            (rest_layer_points[:, 2] - ref_layer_points[:, 2]),\n",
    "                            (rest_layer_points[:, 0] ** 2 - ref_layer_points[:, 0] ** 2),\n",
    "                            (rest_layer_points[:, 1] ** 2 - ref_layer_points[:, 1] ** 2),\n",
    "                            (rest_layer_points[:, 2] ** 2 - ref_layer_points[:, 2] ** 2),\n",
    "                            (rest_layer_points[:, 0] * rest_layer_points[:, 1] - ref_layer_points[:, 0] * ref_layer_points[:, 1]),\n",
    "                            (rest_layer_points[:, 0] * rest_layer_points[:, 2] - ref_layer_points[:, 0] * ref_layer_points[:, 2]),\n",
    "                            (rest_layer_points[:, 1] * rest_layer_points[:, 2] - ref_layer_points[:, 1] * ref_layer_points[:, 2])], 1)\n",
    "\n",
    "            length_of_CG = C_G.shape[1]\n",
    "            length_of_CGI = C_GI.shape[1]\n",
    "            U_G = U_G[:length_of_CG, :3]\n",
    "            U_I = U_I[:length_of_CGI, :3]\n",
    "\n",
    "\n",
    "            U = np.concatenate([U_G,U_I],0)\n",
    "\n",
    "            # build zero matrix\n",
    "            zero_matrix = np.zeros([U.shape[1],U.shape[1]])\n",
    "\n",
    "            # concatenate drift matrix and kriging matrix\n",
    "            K_D = np.concatenate([np.concatenate([K,U],axis = 1),np.concatenate([U.T,zero_matrix],axis = 1)],axis = 0)\n",
    "            # build right side matrix of cokriging system\n",
    "            bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "            bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "            # solve kriging weight\n",
    "            w = np.linalg.lstsq(K_D,bk)[0]\n",
    "            # gradient contribution\n",
    "            sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "\n",
    "            sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "            # surface point contribution\n",
    "            sigma_0_interf = -w[G_1.shape[0]*3:-3]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "            sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "            # 2nd order drift contribution\n",
    "            sigma_0_2nd_drift_1 = np.sum(grid* (w[-3:]).T,axis = 1)\n",
    "            sigma_0_2nd_drift = sigma_0_2nd_drift_1\n",
    "            interpolate_result = 1*sigma_0_grad+1*sigma_0_interf + 1*sigma_0_2nd_drift \n",
    "\n",
    "        intp = interpolate_result  # reshape the result to matrix shape\n",
    "\n",
    "        if cv == True:\n",
    "            return intp\n",
    "        else:\n",
    "            intp = np.reshape(interpolate_result, [50, 50, 50])\n",
    "            return intp\n",
    "    \n",
    "    def systematic_sampling(self, df, n):\n",
    "        count = len(df)\n",
    "        step = max(1, count // n)\n",
    "        indices = list(range(0, count, step))[:n]\n",
    "        return df.iloc[indices]\n",
    "    \n",
    "    def kmeans_sampling_indices(self,df, n_samples, random_state=None):\n",
    "        kmeans = KMeans(n_clusters=n_samples, random_state=random_state)\n",
    "        kmeans.fit(df)\n",
    "        \n",
    "        sampled_indices = []\n",
    "        for i in range(n_samples):\n",
    "            cluster_indices = (kmeans.labels_ == i)\n",
    "            cluster_points = df[cluster_indices]\n",
    "            centroid = kmeans.cluster_centers_[i]\n",
    "            closest_index = cluster_points.apply(lambda row: ((row - centroid) ** 2).sum(), axis=1).idxmin()\n",
    "            sampled_indices.append(closest_index)\n",
    "        \n",
    "        return sampled_indices\n",
    "        \n",
    "\n",
    "    def evaluate_params_kfold(self, a, b, c, eps, range_val, G_position, G_orientation, layer_position, drift_type, n_splits=5):\n",
    "        try:\n",
    "            loo = LeaveOneOut()\n",
    "            kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "            errors = []\n",
    "            for train_index, val_index in kf.split(layer_position):\n",
    "                train_layer = layer_position[train_index]\n",
    "                val_layer = layer_position[val_index]\n",
    "                real_data = train_layer[[0]]\n",
    "\n",
    "                intp = self.RBF_3D_kernel(\n",
    "                    G_position, G_orientation, train_layer, test_data=val_layer, eps=eps, range_val=range_val,\n",
    "                    drift=drift_type, cv=True, a=a, b=b, c=c\n",
    "                )\n",
    "                actual = self.RBF_3D_kernel(\n",
    "                    G_position, G_orientation, train_layer, test_data=real_data, eps=eps, range_val=range_val,\n",
    "                    drift=drift_type, cv=True, a=a, b=b, c=c\n",
    "                )\n",
    "                error = np.mean(np.abs(intp - actual)/np.abs(actual))\n",
    "                errors.append(error)          \n",
    "            avg_error = np.mean(errors)\n",
    "            return (avg_error, (a, b, c, eps, range_val))\n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters a={a}, b={b}, c={c}: {e}\")\n",
    "            return (np.inf, (a, b, c, eps, range_val))  \n",
    "\n",
    "    def run_cross_validation(self):\n",
    "        def squared_euclidean_distance(x_1, x_2):\n",
    "            sqd = np.sqrt(np.reshape(np.sum(x_1**2, 1), newshape=(x_1.shape[0], 1)) +\n",
    "                        np.reshape(np.sum(x_2**2, 1), newshape=(1, x_2.shape[0])) -\n",
    "                        2 * (x_1 @ x_2.T))\n",
    "            return np.nan_to_num(sqd)\n",
    "        num_surface_points = int(self.num_surface_points_cv.get())\n",
    "        dis = squared_euclidean_distance(self.systematic_sampling(self.cv_vertices_df, num_surface_points)[['X','Y','Z']].values, self.systematic_sampling(self.cv_vertices_df, num_surface_points)[['X','Y','Z']].values)\n",
    "        eps = dis.mean()\n",
    "        range_val = dis.max() * 2\n",
    "\n",
    "        try:\n",
    "            a_start = float(self.param_a_start_cv.get()) if self.param_a_start_cv.get() else 1.0\n",
    "            a_stop = float(self.param_a_stop_cv.get()) if self.param_a_stop_cv.get() else 2.0\n",
    "            a_step = float(self.param_a_step_cv.get()) if self.param_a_step_cv.get() else 1.0\n",
    "\n",
    "            b_start = float(self.param_b_start_cv.get()) if self.param_b_start_cv.get() else 1.0\n",
    "            b_stop = float(self.param_b_stop_cv.get()) if self.param_b_stop_cv.get() else 2.0\n",
    "            b_step = float(self.param_b_step_cv.get()) if self.param_b_step_cv.get() else 1.0\n",
    "\n",
    "            c_start = float(self.param_c_start_cv.get()) if self.param_c_start_cv.get() else 1.0\n",
    "            c_stop = float(self.param_c_stop_cv.get()) if self.param_c_stop_cv.get() else 2.0\n",
    "            c_step = float(self.param_c_step_cv.get()) if self.param_c_step_cv.get() else 1.0\n",
    "\n",
    "            eps_start = float(self.epsilon_start_cv.get()) if self.epsilon_start_cv.get() else eps\n",
    "            eps_stop = float(self.epsilon_stop_cv.get()) if self.epsilon_stop_cv.get() else eps*2\n",
    "            eps_step = float(self.epsilon_step_cv.get()) if self.epsilon_step_cv.get() else eps\n",
    "\n",
    "            range_start = float(self.range_start_cv.get()) if self.range_start_cv.get() else range_val\n",
    "            range_stop = float(self.range_stop_cv.get()) if self.range_stop_cv.get() else range_val*2\n",
    "            range_step = float(self.range_step_cv.get()) if self.range_step_cv.get() else range_val\n",
    "\n",
    "\n",
    "            a_values = np.arange(a_start, a_stop, a_step)\n",
    "            b_values = np.arange(b_start, b_stop, b_step)\n",
    "            c_values = np.arange(c_start, c_stop, c_step)\n",
    "            eps_values = np.arange(eps_start, eps_stop, eps_step)\n",
    "            range_values = np.arange(range_start, range_stop, range_step)\n",
    "\n",
    "            param_combinations = list(itertools.product(a_values, b_values, c_values, eps_values, range_values))\n",
    "            total_combinations = len(param_combinations)\n",
    "\n",
    "            best_error = np.inf\n",
    "            best_params = None\n",
    "\n",
    "            self.progress['maximum'] = total_combinations\n",
    "            self.progress['value'] = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            num_surface_points = int(self.num_surface_points_cv.get())\n",
    "            num_orientation_points = int(self.num_orientation_points_cv.get())\n",
    "            random_state = 42\n",
    "\n",
    "\n",
    "            surface_sampled_indices = self.kmeans_sampling_indices(self.cv_vertices_df, num_surface_points, random_state)\n",
    "            if self.Sampling_method_cv.get() == 'Statistic(K-means)':\n",
    "                layer_position = self.cv_vertices_df.iloc[surface_sampled_indices][['X', 'Y', 'Z']].values\n",
    "                G_position = layer_position[:num_orientation_points]\n",
    "                G_orientation = self.cv_normals_df.iloc[surface_sampled_indices][['x', 'y', 'z']].values[:num_orientation_points]\n",
    "            elif self.Sampling_method_cv.get() == 'Random':\n",
    "                layer_position = self.cv_vertices_df.sample(n=num_surface_points, random_state=random_state)[['X', 'Y', 'Z']].values\n",
    "                G_position = self.cv_vertices_df.sample(n=num_orientation_points, random_state=random_state)[['X', 'Y', 'Z']].values\n",
    "                G_orientation = self.cv_normals_df.sample(n=num_orientation_points, random_state=random_state)[['x', 'y', 'z']].values\n",
    "            elif self.Sampling_method_cv.get() == 'Systematic':\n",
    "                layer_position = self.systematic_sampling(self.cv_vertices_df, num_surface_points)[['X', 'Y', 'Z']].values\n",
    "                G_position = self.systematic_sampling(self.cv_vertices_df, num_orientation_points)[['X', 'Y', 'Z']].values\n",
    "                G_orientation = self.systematic_sampling(self.cv_normals_df, num_orientation_points)[['x', 'y', 'z']].values\n",
    "\n",
    "            drift_type_str = self.drift_type_cv.get()\n",
    "            drift_type_map = {\n",
    "                'None': 0,\n",
    "                'Second Order Polynomial': 1,\n",
    "                'Ellipsoid': 2,\n",
    "                'Dome Shaped': 3,\n",
    "                'First Order Polynomial': 4\n",
    "            }\n",
    "            drift_type = drift_type_map.get(drift_type_str, 0)\n",
    "\n",
    "            for idx, (a, b, c, eps, range_val) in enumerate(param_combinations):\n",
    "                error, params = self.evaluate_params_kfold(a, b, c, eps, range_val, G_position, G_orientation, layer_position, drift_type, n_splits=5)\n",
    "                if error < best_error:\n",
    "                    best_error = error\n",
    "                    best_params = params\n",
    "\n",
    "                self.progress['value'] = idx + 1\n",
    "                elapsed_time = time.time() - start_time\n",
    "                estimated_total_time = (elapsed_time / (idx+1)) * total_combinations\n",
    "                remaining_time = estimated_total_time - elapsed_time\n",
    "                self.elapsed_time_label.config(text=f\"time used:{int(elapsed_time)}s\")\n",
    "                self.remaining_time_label.config(text=f\"estimated remaining time:{int(remaining_time)}s\")\n",
    "                self.root.update_idletasks()\n",
    "            \n",
    "            if best_params is not None:\n",
    "                self.result_label.config(\n",
    "                    text = f'Best parameters: eps={best_params[3]:.2f}, range={best_params[4]:.2f}, a={best_params[0]:.2f}, b={best_params[1]:.2f}, c={best_params[2]:.2f}\\nLeast error: {best_error:.2f} %'\n",
    "                )\n",
    "            else:\n",
    "                self.result_label.config(text=\"Cant find best parameters.\")\n",
    "        except Exception as e:\n",
    "            error_message = traceback.format_exc()\n",
    "            messagebox.showerror(\"Error\", f\"Error: {e}\\n\\n{error_message}\")\n",
    "\n",
    "    def compute_rbf_interpolation(self):\n",
    "\n",
    "        def squared_euclidean_distance(x_1, x_2):\n",
    "                    sqd = np.sqrt(np.reshape(np.sum(x_1**2, 1), newshape=(x_1.shape[0], 1)) +\n",
    "                                np.reshape(np.sum(x_2**2, 1), newshape=(1, x_2.shape[0])) -\n",
    "                                2 * (x_1 @ x_2.T))\n",
    "                    return np.nan_to_num(sqd)\n",
    "        \n",
    "        if self.vertices_df is None or self.normals_o_df is None:\n",
    "            messagebox.showerror(\"Error\", \"Please load both surface points and orientations.\")\n",
    "            return\n",
    "        \n",
    "        if self.normals_df is None:\n",
    "            self.normals_df = self.vertices_df\n",
    "            return\n",
    "\n",
    "        try:\n",
    "\n",
    "            seed = 55\n",
    "            num_surface_points = int(self.num_surface_points.get())\n",
    "            num_orientation_points = int(self.num_orientation_points.get())\n",
    "            kernel_function = self.kernel_function.get()\n",
    "            model_color = self.model_color.get()\n",
    "            drift_type_str = self.drift_type.get()\n",
    "\n",
    "\n",
    "            # Default values for eps, range, a, b, c\n",
    "            dis = squared_euclidean_distance(self.systematic_sampling(self.vertices_df, num_surface_points)[['X','Y','Z']].values, self.systematic_sampling(self.vertices_df, num_surface_points)[['X','Y','Z']].values)\n",
    "            default_eps = dis.mean()\n",
    "            default_range = dis.max() * 2\n",
    "            default_a = 1\n",
    "            default_b = 1\n",
    "            default_c = 1\n",
    "\n",
    "            # Use default values if entries are empty\n",
    "            eps = float(self.epsilon_entry.get()) if self.epsilon_entry.get() else default_eps\n",
    "            range_val = float(self.range_entry.get()) if self.range_entry.get() else default_range\n",
    "            a = float(self.param_a_entry.get()) if self.param_a_entry.get() else default_a\n",
    "            b = float(self.param_b_entry.get()) if self.param_b_entry.get() else default_b\n",
    "            c = float(self.param_c_entry.get()) if self.param_c_entry.get() else default_c\n",
    "\n",
    "            \n",
    "\n",
    "            drift_type_map = {\n",
    "                'None': 0,\n",
    "                'Second Order Polynomial': 1,\n",
    "                'Ellipsoid': 2,\n",
    "                'Dome Shaped': 3,\n",
    "                'First Order Polynomial': 4,\n",
    "                '2D Custom' : 5,\n",
    "                '3D Custom' : 6\n",
    "            }\n",
    "            drift_type = drift_type_map[drift_type_str]\n",
    "\n",
    "            def kernel(radius, function, eps=1, range_val=1, a=1, b=1, n=1):\n",
    "                if function == 'cubic':\n",
    "                    return radius**3\n",
    "                elif function == 'gaussian':\n",
    "                    return np.exp(-1 * (radius**2)/ (2* eps**2))\n",
    "                elif function == 'quintic':\n",
    "                    return radius**5\n",
    "                elif function == 'cubic_covariance':\n",
    "                    return (1 - 7 * (radius / range_val) ** 2 +\n",
    "                            35 / 4 * (radius / range_val) ** 3 -\n",
    "                            7 / 2 * (radius / range_val) ** 5 +\n",
    "                            3 / 4 * (radius / range_val) ** 7)\n",
    "                elif function == 'multiquadric':\n",
    "                    return np.sqrt(radius**2+eps**2)\n",
    "                elif function == 'linear':\n",
    "                    return -radius\n",
    "                elif function == 'thin_plate':\n",
    "                    return radius**2 * np.log(radius)\n",
    "                elif function == 'inverse_quadratic':\n",
    "                    return 1 / (eps**2 + radius**2)\n",
    "                elif function == 'inverse_multiquadric':\n",
    "                    return 1 / np.sqrt(radius**2+eps**2)\n",
    "                elif function == 'gaussian_covariance':\n",
    "                    return np.exp(-(radius**2 / range_val**2))\n",
    "                elif function == 'ga_cub_cov':\n",
    "                    return a * (1 - np.exp(-(radius / range_val)**2)) + b * (1 - 7 * (radius / range_val) ** 2 +\n",
    "                        35 / 4 * (radius / range_val) ** 3 - 7 / 2 * (radius / range_val) ** 5 +\n",
    "                        3 / 4 * (radius / range_val) ** 7)\n",
    "                elif function == 'ga_cub':\n",
    "                    return np.exp(-1 * radius**2 / (2 * eps**2)) + n * (radius**3)\n",
    "\n",
    "            # Perform RBF Interpolation\n",
    "            def RBF_3D_kernel(G_position, G_orientation, layer_position, test_data, drift=0, cv=True):\n",
    "                df = egrad(kernel)\n",
    "                ddf = egrad(df)\n",
    "                func = kernel_function\n",
    "                G_1 = G_position\n",
    "                G_1_o = G_orientation\n",
    "                layer1 = layer_position\n",
    "\n",
    "                def squared_euclidean_distance(x_1, x_2):\n",
    "                    sqd = np.sqrt(np.reshape(np.sum(x_1**2, 1), newshape=(x_1.shape[0], 1)) +\n",
    "                                np.reshape(np.sum(x_2**2, 1), newshape=(1, x_2.shape[0])) -\n",
    "                                2 * (x_1 @ x_2.T))\n",
    "                    return np.nan_to_num(sqd)\n",
    "\n",
    "                def cartesian_dist(x_1, x_2):\n",
    "                    return np.concatenate([\n",
    "                        np.tile(x_1[:, 0] - np.reshape(x_2[:, 0], [x_2[:, 0].shape[0], 1]), [1, 3]),\n",
    "                        np.tile(x_1[:, 1] - np.reshape(x_2[:, 1], [x_2[:, 1].shape[0], 1]), [1, 3]),\n",
    "                        np.tile(x_1[:, 2] - np.reshape(x_2[:, 2], [x_2[:, 2].shape[0], 1]), [1, 3])], axis=0)\n",
    "\n",
    "                def cov_gradients(dist_tiled):\n",
    "                    a = (h_u * h_v)\n",
    "                    b = dist_tiled**2\n",
    "\n",
    "                    t1 = np.divide(a, b, out=np.zeros_like(a), casting='unsafe', where=b!=0)\n",
    "                    t2 = np.where(dist_tiled < range_val, ddf(dist_tiled, function=func) - np.nan_to_num(df(dist_tiled, function=func)/dist_tiled), 0)\n",
    "                    t3 = perpendicularity_matrix * np.where(dist_tiled < range_val, (np.nan_to_num(df(dist_tiled, function=func)/dist_tiled)), 0)\n",
    "                    t4 = 1/3 * np.eye(dist_tiled.shape[0])\n",
    "\n",
    "                    C_G = t1 * t2 - t3 + t4\n",
    "                    return C_G\n",
    "\n",
    "                def set_rest_ref_matrix(number_of_points_per_surface):\n",
    "                    ref_layer_points = np.repeat(np.stack([layer1[-1]], axis=0), repeats=number_of_points_per_surface-1, axis=0)\n",
    "                    rest_layer_points = np.concatenate([layer1[0:-1]], axis=0)\n",
    "                    return ref_layer_points, rest_layer_points\n",
    "\n",
    "                def cov_interface(ref_layer_points, rest_layer_points):\n",
    "                    sed_rest_rest = squared_euclidean_distance(rest_layer_points, rest_layer_points)\n",
    "                    sed_ref_rest = squared_euclidean_distance(ref_layer_points, rest_layer_points)\n",
    "                    sed_rest_ref = squared_euclidean_distance(rest_layer_points, ref_layer_points)\n",
    "                    sed_ref_ref = squared_euclidean_distance(ref_layer_points, ref_layer_points)\n",
    "\n",
    "                    C_I = kernel(sed_rest_rest, function=func) - kernel(sed_ref_rest, function=func) - kernel(sed_rest_ref, function=func) + kernel(sed_ref_ref, function=func)\n",
    "                    return C_I\n",
    "\n",
    "                def cartesian_dist_no_tile(x_1, x_2):\n",
    "                    return np.concatenate([\n",
    "                        np.transpose((x_1[:, 0] - np.reshape(x_2[:, 0], [x_2.shape[0], 1]))),\n",
    "                        np.transpose((x_1[:, 1] - np.reshape(x_2[:, 1], [x_2.shape[0], 1]))),\n",
    "                        np.transpose((x_1[:, 2] - np.reshape(x_2[:, 2], [x_2.shape[0], 1])))\n",
    "                    ], axis=0)\n",
    "\n",
    "                def cov_interface_gradients(hu_rest, hu_ref):\n",
    "                    C_GI = hu_rest * np.where(sed_dips_rest < range_val, (np.nan_to_num(df(sed_dips_rest, function=func) / sed_dips_rest)), 0) - \\\n",
    "                        hu_ref * np.where(sed_dips_ref < range_val, (np.nan_to_num(df(sed_dips_ref, function=func) / sed_dips_ref)), 0)\n",
    "                    return C_GI\n",
    "\n",
    "                def perpendicularity(G_1):\n",
    "                    a = np.concatenate([np.ones([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]])], axis=1)\n",
    "                    b = np.concatenate([np.zeros([G_1.shape[0], G_1.shape[0]]), np.ones([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]])], axis=1)\n",
    "                    c = np.concatenate([np.zeros([G_1.shape[0], G_1.shape[0]]), np.zeros([G_1.shape[0], G_1.shape[0]]), np.ones([G_1.shape[0], G_1.shape[0]])], axis=1)\n",
    "                    return np.concatenate([a, b, c], axis=0)\n",
    "                \n",
    "                def plot_3D(grid,value,surfaces_nr=10):\n",
    "                    pv.set_jupyter_backend('static')\n",
    "                    grid = pv.StructuredGrid(XX,YY,ZZ)\n",
    "                    p = pv.Plotter(notebook=True,window_size=[1500,1500])\n",
    "                    p.set_background('white')\n",
    "\n",
    "                    p.add_points(layer1[:,[0,1,2]], render_points_as_spheres=True,point_size=20.0,color='blue')\n",
    "\n",
    "                    grid.point_data['scalar'] = value.ravel(order='F')\n",
    "\n",
    "                    index = np.where(layer_position[:,0] + layer_position[:,1] == (layer_position[:,0] + layer_position[:,1]).min())\n",
    "                    distances = np.sqrt((XX - layer1[index,0])**2 + (YY - layer1[index,1])**2 + (ZZ - layer1[index,2])**2)\n",
    "                    closest_point_indices = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "                    lvl_t = np.array([intp[closest_point_indices]])\n",
    "                    print('surface scalar value:',lvl_t)\n",
    "                    contours_2 = grid.contour(lvl_t)\n",
    "                    p.add_mesh(contours_2, show_scalar_bar=0, label='surface',style='surface',color=model_color,opacity=0.8)\n",
    "\n",
    "                    p.add_bounding_box()\n",
    "\n",
    "                    p.show_bounds(\n",
    "                        grid='front',  \n",
    "                        location='outer',  \n",
    "                        xlabel='X Axis',\n",
    "                        ylabel='Y Axis',\n",
    "                        zlabel='Z Axis',\n",
    "                        color='black',  \n",
    "                        font_size=30  \n",
    "                    )\n",
    "                    p.set_scale(1,1,1.5)\n",
    "                    p.add_arrows((G_1[:,[0,1,2]]),direction=(G_1_o[:,[0,1,2]]),color='black',mag=50)\n",
    "                    p.add_axes()\n",
    "                    p.screenshot()\n",
    "                    p.show()\n",
    "                    return contours_2\n",
    "                \n",
    "                def plot_3D_drift(grid,value,surfaces_nr=10):\n",
    "                    pv.set_jupyter_backend('static')\n",
    "\n",
    "                    grid = pv.StructuredGrid(XX,YY,ZZ)\n",
    "\n",
    "                    p = pv.Plotter(notebook=True,window_size=[400,400])\n",
    "                    p.set_background('white')\n",
    "\n",
    "                    p.add_points(layer1[:,[0,1,2]], render_points_as_spheres=True,point_size=14.0,color='blue')\n",
    "\n",
    "                    grid.point_data['scalar'] = value.ravel(order='F')\n",
    "                    contours_1 = grid.contour(np.linspace(value.min(),value.max(),surfaces_nr))\n",
    "                    p.add_mesh(contours_1, show_scalar_bar=0, label='scalar_field',style='surface',opacity=0.8)\n",
    "                    p.add_arrows((G_1[:,[0,1,2]]),direction=(G_1_o[:,[0,1,2]]),color='black',mag=50)\n",
    "                    p.add_axes()\n",
    "                    p.show()\n",
    "                    return contours_1\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                G_1_tiled = np.tile(G_1, [3, 1])\n",
    "\n",
    "                h_u = cartesian_dist(G_1, G_1)\n",
    "                h_v = h_u.T\n",
    "\n",
    "                perpendicularity_matrix = perpendicularity(G_1)\n",
    "\n",
    "                dist_tiled = squared_euclidean_distance(G_1_tiled, G_1_tiled)\n",
    "\n",
    "                dist_tiled = dist_tiled + np.eye(dist_tiled.shape[0])\n",
    "\n",
    "                C_G = cov_gradients(dist_tiled)\n",
    "\n",
    "                number_of_points_per_surface = np.array([layer1.shape[0]])\n",
    "\n",
    "                ref_layer_points, rest_layer_points = set_rest_ref_matrix(number_of_points_per_surface)\n",
    "\n",
    "                C_I = cov_interface(ref_layer_points, rest_layer_points)\n",
    "\n",
    "                sed_dips_rest = squared_euclidean_distance(G_1_tiled, rest_layer_points)\n",
    "                sed_dips_ref = squared_euclidean_distance(G_1_tiled, ref_layer_points)\n",
    "\n",
    "                hu_rest = cartesian_dist_no_tile(G_1, rest_layer_points)\n",
    "                hu_ref = cartesian_dist_no_tile(G_1, ref_layer_points)\n",
    "\n",
    "                C_GI = cov_interface_gradients(hu_rest, hu_ref)\n",
    "                C_IG = C_GI.T\n",
    "\n",
    "                x0 = (layer1[:, 0].min() + layer1[:, 0].max()) / 2\n",
    "                y0 = (layer1[:, 1].min() + layer1[:, 1].max()) / 2\n",
    "                z0 = (layer1[:, 2].min() + layer1[:, 2].max()) / 2\n",
    "                frame_x = (layer1[:, 0].max() - layer1[:, 0].min()) / 10\n",
    "                frame_y = (layer1[:, 1].max() - layer1[:, 1].min()) / 10\n",
    "                frame_z = (layer1[:, 2].max() - layer1[:, 2].min()) / 2\n",
    "                xx = np.linspace(layer1[:, 0].min() - frame_x, layer1[:, 0].max() + frame_x, 50)\n",
    "                yy = np.linspace(layer1[:, 1].min() - frame_y, layer1[:, 1].max() + frame_y, 50)\n",
    "                zz = np.linspace(layer1[:, 2].min() - frame_z, layer1[:, 2].max() + frame_z, 50)\n",
    "                XX, YY, ZZ = np.meshgrid(xx, yy, zz)\n",
    "                X = np.reshape(XX, [-1]).T\n",
    "                Y = np.reshape(YY, [-1]).T\n",
    "                Z = np.reshape(ZZ, [-1]).T\n",
    "                if cv == True:\n",
    "                    grid = test_data\n",
    "                else:\n",
    "                    grid = np.stack([X, Y, Z], axis=1)\n",
    "\n",
    "                hu_Simpoints = cartesian_dist_no_tile(G_1, grid)\n",
    "                sed_dips_SimPoint = squared_euclidean_distance(G_1_tiled, grid)\n",
    "                sed_rest_SimPoint = squared_euclidean_distance(rest_layer_points, grid)\n",
    "                sed_ref_SimPoint = squared_euclidean_distance(ref_layer_points, grid)\n",
    "\n",
    "                if drift == 1:  # 'Second Order Polynomial'\n",
    "                    K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "                    n = G_1.shape[0]\n",
    "                    sub_x = np.tile(np.array([[1,0,0]]),[n,1])\n",
    "                    sub_y = np.tile(np.array([[0,1,0]]),[n,1])\n",
    "                    sub_z = np.tile(np.array([[0,0,1]]),[n,1])\n",
    "                    sub_block1 = np.concatenate([sub_x,sub_y,sub_z],0)\n",
    "\n",
    "                    sub_x_2 = np.zeros((n, 3))\n",
    "                    sub_y_2 = np.zeros((n, 3))\n",
    "                    sub_z_2 = np.zeros((n, 3))\n",
    "                    sub_x_2[:, 0] = 2 * G_1[:, 0]\n",
    "                    sub_y_2[:, 1] = 2 * G_1[:, 1]\n",
    "                    sub_z_2[:, 2] = 2 * G_1[:, 2]\n",
    "\n",
    "                    sub_block2 = np.concatenate([sub_x_2, sub_y_2, sub_z_2], 0)\n",
    "\n",
    "                    sub_xy = np.reshape(np.concatenate([G_1[:, 1],G_1[:, 0]], 0), [2 * n, 1])\n",
    "                    sub_xy = np.pad(sub_xy, [[0, n], [0, 0]])\n",
    "\n",
    "                    sub_xz = np.concatenate([np.pad(np.reshape(G_1[:, 2], [n, 1]), [[0, n], [0, 0]]), np.reshape(G_1[:, 0], [n, 1])], 0)\n",
    "\n",
    "                    sub_yz = np.reshape(np.concatenate([G_1[:, 2], G_1[:, 1]], 0), [2 * n, 1])\n",
    "                    sub_yz = np.pad(sub_yz, [[n, 0], [0, 0]])\n",
    "\n",
    "                    sub_block3 = np.concatenate([sub_xy, sub_xz, sub_yz], 1)\n",
    "\n",
    "                    U_G = np.concatenate([sub_block1, sub_block2, sub_block3], 1)\n",
    "                    U_I = -np.stack([(rest_layer_points[:, 0] - ref_layer_points[:, 0]), \n",
    "                                    (rest_layer_points[:, 1] - ref_layer_points[:, 1]),\n",
    "                                    (rest_layer_points[:, 2] - ref_layer_points[:, 2]),\n",
    "                                    (rest_layer_points[:, 0] ** 2 - ref_layer_points[:, 0] ** 2),\n",
    "                                    (rest_layer_points[:, 1] ** 2 - ref_layer_points[:, 1] ** 2),\n",
    "                                    (rest_layer_points[:, 2] ** 2 - ref_layer_points[:, 2] ** 2),\n",
    "                                    (rest_layer_points[:, 0] * rest_layer_points[:, 1] - ref_layer_points[:, 0] * ref_layer_points[:, 1]),\n",
    "                                    (rest_layer_points[:, 0] * rest_layer_points[:, 2] - ref_layer_points[:, 0] * ref_layer_points[:, 2]),\n",
    "                                    (rest_layer_points[:, 1] * rest_layer_points[:, 2] - ref_layer_points[:, 1] * ref_layer_points[:, 2])], 1)\n",
    "\n",
    "                    length_of_CG = C_G.shape[1]\n",
    "                    length_of_CGI = C_GI.shape[1]\n",
    "                    U_G = U_G[:length_of_CG, :9]\n",
    "                    U_I = U_I[:length_of_CGI, :9]\n",
    "\n",
    "\n",
    "                    U = np.concatenate([U_G,U_I],0)\n",
    "\n",
    "                    # build zero matrix\n",
    "                    zero_matrix = np.zeros([U.shape[1],U.shape[1]])\n",
    "\n",
    "                    # concatenate drift matrix and kriging matrix\n",
    "                    K_D = np.concatenate([np.concatenate([K,U],axis = 1),np.concatenate([U.T,zero_matrix],axis = 1)],axis = 0)\n",
    "                    # build right side matrix of cokriging system\n",
    "                    bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "                    bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "                    # solve kriging weight\n",
    "                    w = np.linalg.lstsq(K_D,bk)[0]\n",
    "                    # gradient contribution\n",
    "                    sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "                    # surface point contribution\n",
    "                    sigma_0_interf = -w[G_1.shape[0]*3:-9]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "                    # 2nd order drift contribution\n",
    "                    sigma_0_2nd_drift_1 = grid[:,0] * w[-9] + grid[:,1] * w[-8] + grid[:,2] * w[-7] + \\\n",
    "                        grid[:,0] ** 2 * w[-6] + grid[:,1] ** 2 * w[-5] + grid[:,2] ** 2 * w[-4] + \\\n",
    "                        grid[:,0] * grid[:,1] * w[-3] + grid[:,0] * grid[:,2] * w[-2] + grid[:,1] * grid[:,2] * w[-1]\n",
    "                    sigma_0_2nd_drift = sigma_0_2nd_drift_1\n",
    "                    np.save(\"Drift contribution\",sigma_0_2nd_drift )\n",
    "                    np.save(\"Gradient contribution\",sigma_0_grad)\n",
    "                    np.save(\"Interface contribution\",sigma_0_interf)\n",
    "                    interpolate_result = 1*sigma_0_grad+1*sigma_0_interf + 1*sigma_0_2nd_drift \n",
    "                    drift_result = 0*sigma_0_grad+0*sigma_0_interf + 1*sigma_0_2nd_drift\n",
    "\n",
    "                elif drift == 0:  # 'None'\n",
    "                    K_D = np.nan_to_num(np.concatenate([np.concatenate([C_G, C_GI], axis=1), np.concatenate([C_IG, C_I], axis=1)], axis=0))\n",
    "\n",
    "                    bk = np.concatenate([G_1_o[:, 0], G_1_o[:, 1], G_1_o[:, 2], np.zeros(K_D.shape[0] - G_1.shape[0] * 3)], axis=0)\n",
    "                    bk = np.reshape(bk, newshape=[bk.shape[0], 1])\n",
    "\n",
    "                    w = np.linalg.lstsq(K_D, bk, rcond=None)[0]\n",
    "                    sigma_0_grad = w[:G_1.shape[0] * 3] * (-hu_Simpoints * (sed_dips_SimPoint < range_val) * (np.nan_to_num(df(sed_dips_SimPoint, function=func) / sed_dips_SimPoint)))\n",
    "\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad, axis=0)\n",
    "                    sigma_0_interf = -w[G_1.shape[0] * 3:] * (((sed_rest_SimPoint < range_val) * (kernel(sed_rest_SimPoint, function=func)) - (sed_ref_SimPoint < range_val) * (kernel(sed_ref_SimPoint, function=func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf, axis=0)\n",
    "\n",
    "                    interpolate_result = 1 * sigma_0_grad + 1 * sigma_0_interf\n",
    "                    drift_result = 0 * sigma_0_grad + 0 * sigma_0_interf\n",
    "\n",
    "                elif drift == 2:  # 'Spherical'\n",
    "                    x0 = (self.vertices_df[['X']].values.max()-self.vertices_df[['X']].values.min())/2\n",
    "                    y0 = (self.vertices_df[['Y']].values.max()-self.vertices_df[['Y']].values.min())/2\n",
    "                    z0 = (self.vertices_df[['Z']].values.max()-self.vertices_df[['Z']].values.min())/2\n",
    "                    r = 1\n",
    "                    K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "                    D_Z = ((2*(G_1[:,2]-z0)/c)**2).reshape(G_1.shape[0],1)\n",
    "                    D_X = ((2*(G_1[:,1]-x0)/a)**2).reshape(G_1.shape[0],1)\n",
    "                    D_Y = ((2*(G_1[:,0]-y0)/b)**2).reshape(G_1.shape[0],1)\n",
    "\n",
    "                    D_I = ((((ref_layer_points[:,0]-x0)/a)**2+((ref_layer_points[:,1]-y0)/b)**2+((ref_layer_points[:,2]-z0)/c)**2-r**2) \\\n",
    "                        - (((rest_layer_points[:,0]-x0)/a)**2+((rest_layer_points[:,1]-y0)/b)**2+((rest_layer_points[:,2]-z0)/c)**2-r**2)).reshape(-1,1)\n",
    "                \n",
    "                    # build external drift matrix\n",
    "                    D_E = np.concatenate([D_X , D_Y , D_Z , D_I],axis=0)\n",
    "                    D_E_T = D_E.T\n",
    "\n",
    "                    D = D_E\n",
    "                    D_T = D.T \n",
    "\n",
    "                    # build zero matrix\n",
    "                    zero_matrix = np.zeros([D.shape[1],D.shape[1]])\n",
    "                    # concatenate drift matrix and kriging matrix\n",
    "                    K_D = np.concatenate([np.concatenate([K,D],axis = 1),np.concatenate([D_T,zero_matrix],axis = 1)],axis = 0)\n",
    "\n",
    "                    # build right side matrix of cokriging system\n",
    "                    bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "                    bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "\n",
    "                    # solve kriging weight\n",
    "                    w = np.linalg.lstsq(K_D,bk)[0]\n",
    "                    # gradient contribution\n",
    "                    sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "                    # surface point contribution\n",
    "                    sigma_0_interf = -w[G_1.shape[0]*3:-1]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "\n",
    "\n",
    "                    external_drift = (((grid[:,0]-x0)/a)**2+((grid[:,1]-y0)/b)**2+((grid[:,2]-z0)/c)**2+r**2) * (w[-1]).T\n",
    "                    interpolate_result = 1*sigma_0_grad+1*sigma_0_interf   + 1* external_drift\n",
    "                    drift_result = 0*sigma_0_grad+0*sigma_0_interf + 1*external_drift\n",
    "\n",
    "\n",
    "                elif drift == 3:  # 'Dome Shaped'\n",
    "                    K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "                    # z - (c - (x**2 + y**2) / a**2) * np.exp(-(x**2 + y**2) / b**2)\n",
    "\n",
    "                    def F_x(x, y, z, a, b, c):\n",
    "                        return z - (c - ((x - x0)**2 + (y - y0)**2) / a**2) * np.exp(-((x - x0)**2 + (y - y0)**2) / b**2)\n",
    "                    \n",
    "                    def partial_F_x(x, y, a, b, c):\n",
    "                        term1 = (2 * (x - x0) * (c - ((x - x0)**2 + (y - y0)**2) / a**2)) / b**2\n",
    "                        term2 = (2 * (x - x0)) / a**2\n",
    "                        return (term1 + term2) * np.exp(-((x - x0)**2 + (y - y0)**2) / b**2)\n",
    "\n",
    "                    def partial_F_y(y, x, a, b, c):\n",
    "                        term1 = (2 * (y - y0) * (c - ((x - x0)**2 + (y - y0)**2) / a**2)) / b**2\n",
    "                        term2 = (2 * (y - y0)) / a**2\n",
    "                        return (term1 + term2) * np.exp(-((x - x0)**2 + (y - y0)**2) / b**2)\n",
    "\n",
    "                    def partial_F_z():\n",
    "                        return np.ones_like(x)\n",
    "                    \n",
    "                    x = G_1[:, 0]\n",
    "                    y = G_1[:, 1]\n",
    "                    z = G_1[:, 2]\n",
    "\n",
    "                    D_Z = partial_F_z().reshape(G_1.shape[0],1)\n",
    "                    D_X = partial_F_x(x, y, a, b, c).reshape(G_1.shape[0],1)\n",
    "                    D_Y = partial_F_y(y, x, a, b, c).reshape(G_1.shape[0],1)\n",
    "\n",
    "                    D_I = (F_x(ref_layer_points[:,0], ref_layer_points[:,1], ref_layer_points[:,2], a, b, c) \\\n",
    "                        - F_x(rest_layer_points[:,0], rest_layer_points[:,1], rest_layer_points[:,2], a, b, c)).reshape(-1,1)\n",
    "                    # build external drift matrix\n",
    "                    D = np.concatenate([D_X , D_Y , D_Z , D_I],axis=0)\n",
    "\n",
    "                    D_T = D.T \n",
    "\n",
    "                    # build zero matrix\n",
    "                    zero_matrix = np.zeros([D.shape[1],D.shape[1]])\n",
    "                    # concatenate drift matrix and kriging matrix\n",
    "                    K_D = np.concatenate([np.concatenate([K,D],axis = 1),np.concatenate([D_T,zero_matrix],axis = 1)],axis = 0)\n",
    "\n",
    "                    # build right side matrix of cokriging system\n",
    "                    bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "                    bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "\n",
    "                    # solve kriging weight\n",
    "                    w = np.linalg.lstsq(K_D,bk)[0]\n",
    "                    # gradient contribution\n",
    "                    sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "                    # surface point contribution\n",
    "                    sigma_0_interf = -w[G_1.shape[0]*3:-1]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "\n",
    "\n",
    "                    external_drift = F_x(grid[:,0],grid[:,1],grid[:,2],a,b,c) * (w[-1]).T\n",
    "                    interpolate_result = 1*sigma_0_grad+1*sigma_0_interf   + 1* external_drift\n",
    "                    drift_result = 0*sigma_0_grad+0*sigma_0_interf + 1*external_drift\n",
    "\n",
    "                elif drift == 4:  # 'First Order Polynomial'\n",
    "                    K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "                    n = G_1.shape[0]\n",
    "                    sub_x = np.tile(np.array([[1,0,0]]),[n,1])\n",
    "                    sub_y = np.tile(np.array([[0,1,0]]),[n,1])\n",
    "                    sub_z = np.tile(np.array([[0,0,1]]),[n,1])\n",
    "                    sub_block1 = np.concatenate([sub_x,sub_y,sub_z],0)\n",
    "\n",
    "                    sub_x_2 = np.zeros((n, 3))\n",
    "                    sub_y_2 = np.zeros((n, 3))\n",
    "                    sub_z_2 = np.zeros((n, 3))\n",
    "                    sub_x_2[:, 0] = 2 * G_1[:, 0]\n",
    "                    sub_y_2[:, 1] = 2 * G_1[:, 1]\n",
    "                    sub_z_2[:, 2] = 2 * G_1[:, 2]\n",
    "\n",
    "                    sub_block2 = np.concatenate([sub_x_2, sub_y_2, sub_z_2], 0)\n",
    "\n",
    "                    sub_xy = np.reshape(np.concatenate([G_1[:, 1],G_1[:, 0]], 0), [2 * n, 1])\n",
    "                    sub_xy = np.pad(sub_xy, [[0, n], [0, 0]])\n",
    "\n",
    "                    sub_xz = np.concatenate([np.pad(np.reshape(G_1[:, 2], [n, 1]), [[0, n], [0, 0]]), np.reshape(G_1[:, 0], [n, 1])], 0)\n",
    "\n",
    "                    sub_yz = np.reshape(np.concatenate([G_1[:, 2], G_1[:, 1]], 0), [2 * n, 1])\n",
    "                    sub_yz = np.pad(sub_yz, [[n, 0], [0, 0]])\n",
    "\n",
    "                    sub_block3 = np.concatenate([sub_xy, sub_xz, sub_yz], 1)\n",
    "\n",
    "                    U_G = np.concatenate([sub_block1, sub_block2, sub_block3], 1)\n",
    "                    U_I = -np.stack([(rest_layer_points[:, 0] - ref_layer_points[:, 0]), \n",
    "                                    (rest_layer_points[:, 1] - ref_layer_points[:, 1]),\n",
    "                                    (rest_layer_points[:, 2] - ref_layer_points[:, 2]),\n",
    "                                    (rest_layer_points[:, 0] ** 2 - ref_layer_points[:, 0] ** 2),\n",
    "                                    (rest_layer_points[:, 1] ** 2 - ref_layer_points[:, 1] ** 2),\n",
    "                                    (rest_layer_points[:, 2] ** 2 - ref_layer_points[:, 2] ** 2),\n",
    "                                    (rest_layer_points[:, 0] * rest_layer_points[:, 1] - ref_layer_points[:, 0] * ref_layer_points[:, 1]),\n",
    "                                    (rest_layer_points[:, 0] * rest_layer_points[:, 2] - ref_layer_points[:, 0] * ref_layer_points[:, 2]),\n",
    "                                    (rest_layer_points[:, 1] * rest_layer_points[:, 2] - ref_layer_points[:, 1] * ref_layer_points[:, 2])], 1)\n",
    "\n",
    "                    length_of_CG = C_G.shape[1]\n",
    "                    length_of_CGI = C_GI.shape[1]\n",
    "                    U_G = U_G[:length_of_CG, :3]\n",
    "                    U_I = U_I[:length_of_CGI, :3]\n",
    "\n",
    "\n",
    "                    U = np.concatenate([U_G,U_I],0)\n",
    "\n",
    "                    # build zero matrix\n",
    "                    zero_matrix = np.zeros([U.shape[1],U.shape[1]])\n",
    "\n",
    "                    # concatenate drift matrix and kriging matrix\n",
    "                    K_D = np.concatenate([np.concatenate([K,U],axis = 1),np.concatenate([U.T,zero_matrix],axis = 1)],axis = 0)\n",
    "                    # build right side matrix of cokriging system\n",
    "                    bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "                    bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "                    # solve kriging weight\n",
    "                    w = np.linalg.lstsq(K_D,bk)[0]\n",
    "                    # print(w.shape)\n",
    "                    # gradient contribution\n",
    "                    sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "                    # surface point contribution\n",
    "                    sigma_0_interf = -w[G_1.shape[0]*3:-3]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "                    # 2nd order drift contribution\n",
    "                    sigma_0_2nd_drift_1 = np.sum(grid* (w[-3:]).T,axis = 1)\n",
    "                    sigma_0_2nd_drift = sigma_0_2nd_drift_1\n",
    "                    np.save(\"Drift contribution\",sigma_0_2nd_drift )\n",
    "                    np.save(\"Gradient contribution\",sigma_0_grad)\n",
    "                    np.save(\"Interface contribution\",sigma_0_interf)\n",
    "                    interpolate_result = 1*sigma_0_grad+1*sigma_0_interf + 1*sigma_0_2nd_drift \n",
    "                    drift_result = 0*sigma_0_grad+0*sigma_0_interf + 1*sigma_0_2nd_drift\n",
    "\n",
    "                elif drift == 5:  # '2D custom'\n",
    "                    gravity, aa, bb, cc, dd, ee, ff = self.gravity_fwd_2d()                  \n",
    "                    def F_x(x, y, aa, bb, cc, dd, ee, ff):\n",
    "                        return aa * x**2 + bb * y**2 + cc * x * y + dd * x + ee * y + ff\n",
    "                    \n",
    "                    def partial_F_x(x, y, aa, bb, cc, dd, ee, ff):\n",
    "                        return 2 * aa * x + cc * y + dd\n",
    "                    \n",
    "                    def partial_F_y(x, y, aa, bb, cc, dd, ee, ff):\n",
    "                        return 2 * bb * y + cc * x + ee\n",
    "\n",
    "                    K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "\n",
    "                    x = G_1[:, 0]\n",
    "                    y = G_1[:, 1]\n",
    "                    z = G_1[:, 2]\n",
    "\n",
    "                    D_X = partial_F_x(x, y, aa, bb, cc, dd, ee, ff).reshape(G_1.shape[0],1)\n",
    "                    D_Y = partial_F_y(x, y, aa, bb, cc, dd, ee, ff).reshape(G_1.shape[0],1)\n",
    "                    D_Z = np.ones_like(x).reshape(G_1.shape[0],1) * 0\n",
    "                    D_I = (F_x(ref_layer_points[:,0], ref_layer_points[:,1], aa, bb, cc, dd, ee, ff) \\\n",
    "                        - F_x(rest_layer_points[:,0], rest_layer_points[:,1], aa, bb, cc, dd, ee, ff)).reshape(-1,1)\n",
    "                \n",
    "                    # build external drift matrix\n",
    "                    D_E = np.concatenate([D_X , D_Y , D_Z , D_I],axis=0)\n",
    "                    D = D_E\n",
    "                    D_T = D.T \n",
    "\n",
    "                    # build zero matrix\n",
    "                    zero_matrix = np.zeros([D.shape[1],D.shape[1]])\n",
    "                    # concatenate drift matrix and kriging matrix\n",
    "                    K_D = np.concatenate([np.concatenate([K,D],axis = 1),np.concatenate([D_T,zero_matrix],axis = 1)],axis = 0)\n",
    "\n",
    "                    # build right side matrix of cokriging system\n",
    "                    bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "                    bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "\n",
    "                    # solve kriging weight\n",
    "                    w = np.linalg.lstsq(K_D,bk)[0]\n",
    "                    # gradient contribution\n",
    "                    sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "                    # surface point contribution\n",
    "                    sigma_0_interf = -w[G_1.shape[0]*3:-1]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "\n",
    "                    \n",
    "                    external_drift = F_x(grid[:,0],grid[:,1],aa,bb,cc,dd,ee,ff) * (w[-1]).T\n",
    "                    interpolate_result = 1*sigma_0_grad+1*sigma_0_interf   + 1* external_drift\n",
    "                    drift_result = 0*sigma_0_grad+0*sigma_0_interf + 1*external_drift\n",
    "\n",
    "                elif drift == 6:  # '3D custom'\n",
    "\n",
    "                    def numerical_gradient(F, x, eps=1e-5):\n",
    "                        # F is a scalar function from R^n -> R\n",
    "                        # x is a point in R^n\n",
    "                        grad = np.zeros_like(x, dtype=float)\n",
    "                        for i in range(len(x)):\n",
    "                            x_plus = x.copy();   x_plus[i] += eps\n",
    "                            x_minus = x.copy();  x_minus[i] -= eps\n",
    "                            grad[i] = (F(x_plus) - F(x_minus)) / (2*eps)\n",
    "                        return grad\n",
    "\n",
    "                    \n",
    "                    K = np.concatenate([np.concatenate([C_G,C_GI],axis = 1),np.concatenate([C_IG,C_I],axis = 1)],axis = 0)\n",
    "                    scalar_field = self.drift_scalar \n",
    "                    scalar_field *= float(self.scaling_factor.get())\n",
    "                    scalar_field =  scalar_field.transpose(2, 1, 0)\n",
    "\n",
    "                    interpolator = RegularGridInterpolator((xx,yy,zz), scalar_field,method='linear')\n",
    "                    def F(point):\n",
    "                        return interpolator(point.reshape(1, -1))[0]   \n",
    "                    def F_x(points):\n",
    "                        return interpolator(points) \n",
    "\n",
    "                    gradient_F = egrad(F)  \n",
    "\n",
    "                    points = G_1  \n",
    "\n",
    "                    gradients = np.array([numerical_gradient(F, p) for p in points])\n",
    "\n",
    "                    D_X = gradients[:, 0].reshape(-1, 1)\n",
    "                    D_Y = gradients[:, 1].reshape(-1, 1)\n",
    "                    D_Z = gradients[:, 2].reshape(-1, 1)\n",
    "                    D_I = (F_x(np.array(ref_layer_points) ) - F_x(np.array(rest_layer_points))).reshape(-1, 1)\n",
    "                    # build external drift matrix\n",
    "                    D = np.concatenate([D_X , D_Y , D_Z , D_I],axis=0)\n",
    "                    D_T = D.T \n",
    "                    # build zero matrix\n",
    "                    zero_matrix = np.zeros([D.shape[1],D.shape[1]])\n",
    "                    # concatenate drift matrix and kriging matrix\n",
    "                    K_D = np.concatenate([np.concatenate([K,D],axis = 1),np.concatenate([D_T,zero_matrix],axis = 1)],axis = 0)\n",
    "\n",
    "                    # build right side matrix of cokriging system\n",
    "                    bk = np.concatenate([G_1_o[:,0],G_1_o[:,1],G_1_o[:,2],np.zeros(K_D.shape[0]-G_1.shape[0]*3)],axis = 0)\n",
    "                    bk = np.reshape(bk,newshape = [bk.shape[0],1])\n",
    "\n",
    "                    # solve kriging weight\n",
    "                    w = np.linalg.lstsq(K_D,bk)[0]\n",
    "                    # gradient contribution\n",
    "                    sigma_0_grad = w[:G_1.shape[0]*3] * (-hu_Simpoints*(sed_dips_SimPoint < range_val)*(np.nan_to_num(df(sed_dips_SimPoint,function = func)/sed_dips_SimPoint)))\n",
    "\n",
    "                    sigma_0_grad = np.sum(sigma_0_grad,axis=0)\n",
    "                    # surface point contribution\n",
    "                    sigma_0_interf = -w[G_1.shape[0]*3:-1]*(((sed_rest_SimPoint < range_val)*(kernel(sed_rest_SimPoint,function = func)) - (sed_ref_SimPoint < range_val)*(kernel(sed_ref_SimPoint,function = func))))\n",
    "                    sigma_0_interf = np.sum(sigma_0_interf,axis = 0)\n",
    "\n",
    "\n",
    "                    external_drift = F_x(np.array(grid)) * (w[-1]).T\n",
    "\n",
    "                    # np.save(\"External drift contribution\",external_drift)\n",
    "                    # np.save(\"Gradient contribution\",sigma_0_grad)\n",
    "                    # np.save(\"Interface contribution\",sigma_0_interf)\n",
    "\n",
    "                    # print(\"External drift contribution:\", np.max(external_drift), np.min(external_drift))\n",
    "                    # print(\"Gradient contribution:\", np.max(sigma_0_grad), np.min(sigma_0_grad))\n",
    "                    # print(\"Interface contribution:\", np.max(sigma_0_interf), np.min(sigma_0_interf))\n",
    "\n",
    "                    interpolate_result = 1*sigma_0_grad+1*sigma_0_interf   + 1* external_drift\n",
    "                    drift_result = 0*sigma_0_grad+0*sigma_0_interf + 1*external_drift\n",
    "\n",
    "                print(\"weight:\", w)\n",
    "                intp = interpolate_result\n",
    "                drift_contribution = drift_result\n",
    "                if cv == True:\n",
    "                    return intp\n",
    "                else:\n",
    "                    show_drift_var = self.show_drift_var.get()\n",
    "                    if show_drift_var == 0:\n",
    "                        intp = np.reshape(interpolate_result, [50, 50, 50])\n",
    "                        mesh = plot_3D(grid, intp, surfaces_nr=2)\n",
    "                        return intp, mesh\n",
    "                    elif show_drift_var == 1:\n",
    "                        drift_contribution = np.reshape(drift_contribution, [50, 50, 50])                    \n",
    "                        mesh_drift = plot_3D_drift(grid, drift_contribution, surfaces_nr=10)                    \n",
    "                        return drift_contribution, mesh_drift\n",
    "\n",
    "            def systematic_sampling(df, n):\n",
    "                count = len(df)\n",
    "                step = max(1, count // n)\n",
    "                indices = list(range(0, count, step))[:n]\n",
    "                return df.iloc[indices]\n",
    "            \n",
    "            def kmeans_sampling_indices(df, n_samples,random_state=None):\n",
    "                kmeans = KMeans(n_clusters=n_samples, random_state=random_state)\n",
    "                kmeans.fit(df)\n",
    "                \n",
    "                sampled_indices = []\n",
    "                for i in range(n_samples):\n",
    "                    cluster_indices = (kmeans.labels_ == i)\n",
    "                    cluster_points = df[cluster_indices]\n",
    "                    \n",
    "                    centroid = kmeans.cluster_centers_[i]\n",
    "                    closest_index = cluster_points.apply(lambda row: ((row - centroid) ** 2).sum(), axis=1).idxmin()\n",
    "                    sampled_indices.append(closest_index)\n",
    "                \n",
    "                return sampled_indices\n",
    "            random_state = 24\n",
    "            surface_sampled_indices = kmeans_sampling_indices(self.vertices_df, num_surface_points, random_state)\n",
    "\n",
    "            if self.Sampling_method.get() == 'Statistic(K-means)':\n",
    "                layer_position = self.vertices_df.iloc[surface_sampled_indices][['X', 'Y', 'Z']].values\n",
    "                G_position = layer_position[:num_orientation_points]\n",
    "                G_orientation = self.normals_o_df.iloc[surface_sampled_indices][['x', 'y', 'z']].values[:num_orientation_points]\n",
    "            elif self.Sampling_method.get() == 'Random':\n",
    "                layer_position = self.vertices_df.sample(n=num_surface_points, random_state=random_state)[['X', 'Y', 'Z']].values\n",
    "                G_position = self.normals_df.sample(n=num_orientation_points, random_state=random_state)[['X', 'Y', 'Z']].values\n",
    "                G_orientation = self.normals_o_df.sample(n=num_orientation_points, random_state=random_state)[['x', 'y', 'z']].values\n",
    "            elif self.Sampling_method.get() == 'Systematic':\n",
    "                layer_position = systematic_sampling(self.vertices_df, num_surface_points)[['X', 'Y', 'Z']].values\n",
    "                G_position = systematic_sampling(self.normals_df, num_orientation_points)[['X', 'Y', 'Z']].values\n",
    "                G_orientation = systematic_sampling(self.normals_o_df, num_orientation_points)[['x', 'y', 'z']].values\n",
    "\n",
    "            start_time = time.time()\n",
    "            intp, mesh = RBF_3D_kernel(G_position=G_position, \n",
    "                                                                        G_orientation=G_orientation,\n",
    "                                                                        layer_position=layer_position, \n",
    "                                                                        test_data=None, drift=drift_type,\n",
    "                                                                        cv=False) \n",
    "            cost_time = time.time() - start_time\n",
    "            print('cost time:', cost_time)\n",
    "\n",
    "            # Plot in a PyVista window\n",
    "            def plot():\n",
    "                show_drift_var = self.show_drift_var.get()\n",
    "                only_show_data = self.only_show_data.get()\n",
    "                plotter = pv.Plotter(notebook=False)\n",
    "                if only_show_data == 0:\n",
    "                    if show_drift_var == 0:\n",
    "                        plotter.add_mesh(mesh,show_scalar_bar=False,color=model_color,opacity=0.8)\n",
    "                    elif show_drift_var == 1:\n",
    "                        plotter.add_mesh(mesh,show_scalar_bar=False,opacity=0.8)\n",
    "                plotter.add_points(layer_position[:,[0,1,2]], render_points_as_spheres=True,point_size=14.0,color='blue')\n",
    "                plotter.add_arrows((G_position[:,[0,1,2]]),direction=(G_orientation[:,[0,1,2]]),color='black',mag=50)\n",
    "                plotter.set_background('white')\n",
    "                plotter.show()\n",
    "            plot_thread = threading.Thread(target=plot)\n",
    "            plot_thread.start()\n",
    "\n",
    "            # Prompt user to save the mesh\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                initialfile=(\n",
    "                    self.model_name + \"_\" +\n",
    "                    self.Sampling_method.get() + \"_\" +\n",
    "                    self.num_surface_points.get() + \"_\" +\n",
    "                    self.num_orientation_points.get() + \"_\" +\n",
    "                    self.kernel_function.get() + \"_\" +\n",
    "                    self.drift_type.get() + \"_\" +\n",
    "                    self.epsilon_entry.get() + \"_\" +\n",
    "                    self.range_entry.get() + \"_\" +\n",
    "                    self.param_a_entry.get() + \"_\" +\n",
    "                    self.param_b_entry.get() + \"_\" +\n",
    "                    self.param_c_entry.get() +\n",
    "                    \".vtk\"\n",
    "                ),\n",
    "                defaultextension=\".vtk\",\n",
    "                filetypes=[(\"VTK files\", \"*.vtk\")]\n",
    "            )\n",
    "\n",
    "            if file_path:\n",
    "                file_path_scalar = file_path.replace(\".vtk\", \"_scalar.npy\")\n",
    "                mesh.save(file_path)\n",
    "                np.save(file_path_scalar, self.vtk_to_scalar(mesh))\n",
    "                messagebox.showinfo(\"Save Mesh and Compute Scalar Field \", \"Mesh and Scalar Field saved successfully!\")\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = traceback.format_exc()\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred: {e}\\n\\n{error_message}\")\n",
    "\n",
    "    def load_vtk_to_gravity(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"VTK files\", \"*.vtp\"), (\"VTK files\", \"*.vtk\")])\n",
    "        if file_path:\n",
    "            self.mesh = pv.read(file_path)\n",
    "            messagebox.showinfo(\"VTK Operations\", \"VTK file loaded successfully!\")\n",
    "\n",
    "    def my_point_gravity(self,coordinates, points, masses, field=\"g_z\"):\n",
    "        \"\"\"\n",
    "        Compute gravitational acceleration at given observation points due to point masses.\n",
    "\n",
    "        Parameters:\n",
    "        - coordinates: tuple of arrays (easting, northing, upward), each of shape (N,)\n",
    "        - points: tuple of arrays (x_mass, y_mass, z_mass), each of shape (M,)\n",
    "        - masses: array of shape (M,)\n",
    "        - field: str, which component of the gravitational acceleration to compute ('g_z')\n",
    "\n",
    "        Returns:\n",
    "        - gravity: array of shape (N,), gravitational acceleration at observation points\n",
    "        \"\"\"\n",
    "        G = 6.67430e-11  # Gravitational constant in m^3 kg^-1 s^-2\n",
    "\n",
    "        x_obs, y_obs, z_obs = coordinates\n",
    "        x_mass, y_mass, z_mass = points\n",
    "\n",
    "        # Ensure inputs are numpy arrays\n",
    "        x_obs = np.asarray(x_obs)\n",
    "        y_obs = np.asarray(y_obs)\n",
    "        z_obs = np.asarray(z_obs)\n",
    "        x_mass = np.asarray(x_mass)\n",
    "        y_mass = np.asarray(y_mass)\n",
    "        z_mass = np.asarray(z_mass)\n",
    "        masses = np.asarray(masses)\n",
    "\n",
    "        # Compute differences between observation points and masses\n",
    "        dx = x_mass[np.newaxis, :] - x_obs[:, np.newaxis]  # Shape (N, M)\n",
    "        dy = y_mass[np.newaxis, :] - y_obs[:, np.newaxis]\n",
    "        dz = z_mass[np.newaxis, :] - z_obs[:, np.newaxis]\n",
    "        r_squared = dx**2 + dy**2 + dz**2\n",
    "        r = np.sqrt(r_squared)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        r[r < 1e-10] = 1e-10\n",
    "\n",
    "        # Compute gravitational acceleration components\n",
    "        if field == 'g_z':\n",
    "            # Vertical component\n",
    "            g_contributions = G * masses / r**3 * dz\n",
    "        elif field == 'g_x':\n",
    "            # Easting component\n",
    "            g_contributions = G * masses / r**3 * dx\n",
    "        elif field == 'g_y':\n",
    "            # Northing component\n",
    "            g_contributions = G * masses / r**3 * dy\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown field component '{field}'\")\n",
    "\n",
    "        # Sum contributions from all masses\n",
    "        gravity = np.sum(g_contributions, axis=1)\n",
    "\n",
    "        return gravity\n",
    "    \n",
    "    def my_grid_coordinates(self,region, spacing):\n",
    "        \"\"\"\n",
    "        Generate coordinate grids over a specified region with given spacing.\n",
    "\n",
    "        Parameters:\n",
    "        - region: tuple (xmin, xmax, ymin, ymax)\n",
    "        - spacing: float, spacing between grid points\n",
    "\n",
    "        Returns:\n",
    "        - coordinates: tuple of arrays (easting, northing)\n",
    "        \"\"\"\n",
    "        xmin, xmax, ymin, ymax = region\n",
    "        # Create arrays of easting and northing coordinates\n",
    "        easting = np.arange(xmin, xmax + spacing, spacing)\n",
    "        northing = np.arange(ymin, ymax + spacing, spacing)\n",
    "        # Create a meshgrid from the coordinate arrays\n",
    "        easting, northing = np.meshgrid(easting, northing)\n",
    "        return easting, northing\n",
    "\n",
    "    def gravity_sim(self):\n",
    "        geological_model = self.mesh\n",
    "        points = geological_model.points\n",
    "        xmin, xmax = np.min(points[:, 0]), np.max(points[:, 0])\n",
    "        ymin, ymax = np.min(points[:, 1]), np.max(points[:, 1])\n",
    "        density = 800  \n",
    "        masses = np.full(points.shape[0], density)  \n",
    "        coordinates = self.my_grid_coordinates(region=(xmin, xmax, ymin, ymax), spacing=100)\n",
    "        easting, northing = coordinates\n",
    "        upward = np.zeros_like(easting)  \n",
    "        easting = easting.ravel()\n",
    "        northing = northing.ravel()\n",
    "        upward = upward.ravel()\n",
    "        gravity = self.my_point_gravity(\n",
    "            coordinates=(easting, northing, upward),\n",
    "            points=(points[:, 0], points[:, 1], points[:, 2]),\n",
    "            masses=masses,\n",
    "            field=\"g_z\"\n",
    "        )\n",
    "        return gravity, easting, northing\n",
    "    \n",
    "    def gravity_fwd_2d(self):\n",
    "        gravity, easting, northing = self.gravity_sim()\n",
    "        easting_mean = easting.mean()\n",
    "        easting_std = easting.std()\n",
    "        northing_mean = northing.mean()\n",
    "        northing_std = northing.std()\n",
    "        gravity_mean = gravity.mean()\n",
    "        gravity_std = gravity.std()\n",
    "\n",
    "        easting_norm = (easting - easting_mean) / easting_std\n",
    "        northing_norm = (northing - northing_mean) / northing_std\n",
    "        gravity_norm = (gravity - gravity_mean) / gravity_std\n",
    "        def polynomial_gravity_anomaly(easting, northing, params):\n",
    "            a, b, c, d, e, f = params\n",
    "            gravity_anomaly = (\n",
    "                a * easting**2 +\n",
    "                b * northing**2 +\n",
    "                c * easting * northing +\n",
    "                d * easting +\n",
    "                e * northing +\n",
    "                f\n",
    "            )\n",
    "            return gravity_anomaly\n",
    "        def misfit(params, easting, northing, gravity_observed, alpha=1e-6):\n",
    "            gravity_calculated = polynomial_gravity_anomaly(easting, northing, params)\n",
    "            residuals = gravity_observed - gravity_calculated\n",
    "            regularization = alpha * np.sum(params**2)\n",
    "            return np.sum(residuals**2) + regularization\n",
    "\n",
    "        initial_params = np.zeros(6)\n",
    "\n",
    "        result = minimize(\n",
    "            misfit,\n",
    "            initial_params,\n",
    "            args=(easting_norm, northing_norm, gravity_norm),\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': 10000, 'ftol': 1e-12}\n",
    "        )\n",
    "\n",
    "        a_opt, b_opt, c_opt, d_opt, e_opt, f_opt = result.x\n",
    "\n",
    "        gravity_calculated_norm = polynomial_gravity_anomaly(easting_norm, northing_norm, result.x)\n",
    "        gravity_calculated = gravity_calculated_norm * gravity_std + gravity_mean\n",
    "        self.result_label.config(\n",
    "                    text = f'Best parameters:  a={a_opt:.2f}, b={b_opt:.2f}, c={c_opt:.2f}, d={d_opt:.2f}, e={e_opt:.2f}, f={f_opt:.2f}'\n",
    "                )\n",
    "\n",
    "        fig = plt.figure(figsize=(14,6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.tricontourf(easting, northing, gravity, levels=50, cmap='viridis')\n",
    "        plt.colorbar(label='Gravity disturbance (mGal)')\n",
    "        plt.xlabel('Easting [m]')\n",
    "        plt.ylabel('Northing [m]')\n",
    "        plt.title('Observed Gravity Disturbance')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.tricontourf(easting, northing, gravity_calculated, levels=50, cmap='viridis')\n",
    "        plt.colorbar(label='Gravity disturbance (mGal)')\n",
    "        plt.xlabel('Easting [m]')\n",
    "        plt.ylabel('Northing [m]')\n",
    "        plt.title('Fitted Gravity Disturbance (2nd Order Polynomial)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        canvas4 = FigureCanvasTkAgg(fig, master=self.gravity_canvas_frame)\n",
    "        canvas4.draw()\n",
    "        canvas4.get_tk_widget().grid(row=4, column=0, padx=5, pady=5)\n",
    "\n",
    "        return gravity_calculated, a_opt, b_opt, c_opt, d_opt, e_opt, f_opt\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Interpolation Method Comparison Tool\") \n",
    "    app = GeologicalModelApp(root)\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gempy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
